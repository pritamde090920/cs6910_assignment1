{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ig64x9DVVeI"
      },
      "source": [
        "Question 1:\n",
        "\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "Ru0Y5IvRj0R9",
        "outputId": "52866de4-95f4-43e6-b20c-4296ddc054c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAH9CAYAAADLWM92AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWJUlEQVR4nO39eZBdZ33g/9/b+6puSS1rXyx5kW1FGGwIGNvYOBB2YpbAQEimUqQSyAyEYktmqBQphpAKUwOESSVhMkUY1pkMeAYCBjJmR9jG2MKWN1k2siVr7ZZavS+37/3+xa/mN+F+PrKO+7RkXq9/33rOPffc85xz7qNb1dVGo9GoAAAAAECJWpZ6BwAAAAD45WNRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKF3b6f7DarW6mPtRWLZ/jUajpD35l7Zv3x72//yf/3PY//Ef/zHsd911V9jn5ubCPj8/H/YdO3aE/cYbbwz7ww8/HPYPf/jDYR8dHQ37U1mR8/Zsn7NL6corrwz77/zO74R9ZGQk7OPj42Gv1WphHxoaCnt2Xjz22GNhf9rTnhb21atXh33VqlVhv/7668P+VHYuz9mz+T563nnnhf35z39+2N/85jeHPbvP3H///WHP7rODg4Nhv+qqq8J+6623hv3f/bt/F/bp6emwF3E2nzen41yes0tpy5YtYb/uuuvC/spXvjLs2X32M5/5TNjvvPPOsGfP5q9+9avDfsMNN4R9amoq7Nn+f+ITnwj7LzNz9hdraYl/a1Kv1wttv6+vL+yXXXZZ2C+99NKw33PPPWGfmZkJ+7p168J+9OjRsP/0pz8Ne+ZcvxcuptN5734pBQAAAEDpLEoBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlK7aaDQap/UPq9XF3ZFk+6e5m2fk8ssvD/vrX//6sL/61a8O+8LCQth7e3vD3t3dHfaVK1eGfbHt3bs37PV6PewXX3xx2I8ePdq0feMb3wjH/sf/+B/DvmfPnrAvtSLn/WLP2XPZu9/97rC/5CUvCXt2Tp9//vlh7+/vD/vQ0FDYT5w4EfZTp06FfXR0NOwjIyNhv+CCC8Kevf+nsrN5zi72fTY6b9/+9reHY3/t134t7J2dnWGfnJwsNH779u1hz+ZsZn5+PuwHDx4M++HDh8OePSdk14zvfe97TdvHP/7xcOzJkyfDfrY7m+fsYnvxi1/ctL3jHe8Ix05PT4e9o6Mj7DMzM2HP5tyOHTvCvnr16rDv378/7LVaLezZnMzuw9k1af369WG/5ZZbmra3ve1t4dhz3S/znF1M2fexbE5ecsklYb/iiivC/v3vfz/s2X1s1apVYc+uOY899ljYd+/eHXaaO50565dSAAAAAJTOohQAAAAApbMoBQAAAEDpLEoBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6aqNRqNxWv+wWl3sfSlk2bJlYf9v/+2/NW07d+4Mx7a0xGt34+PjYZ+ZmQn7/Px82BcWFsLe3t4e9oGBgbBPTk6GvV6vh/00T6Ez1tXV1bR1d3eHYzs6OsL+/e9/P+xvetObwr7Yihzbs33OLqX3v//9Yd+4cWPYV65cGfYVK1aEvehnE82J09n+6Oho2EdGRsJ+9dVXh/25z31u07Z///5w7LnubJ6z2fazfd+2bVvYv/KVrzRtR48eDccu9n1ydnY27CdOnAh7X1/for5+dq9atWpV2Nva2gptP+pTU1Ph2L/9278N+0033RT2pXY2z9misjkb3QuzOdvT0xP27Nk5e7as1Wphz+7Tmez1s37q1KmwZ/ufXdOya9L69eubtuwe/653vSvsZ7un8pxdTNn1YMuWLWF/9NFHw/6qV70q7Oeff37YP/vZz4Y9e37M3l/2bDs4OBj27DnljjvuCPsvs9OZs34pBQAAAEDpLEoBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlK5tqXfgyfKlL30p7Js3b27ajh07Fo6t1+thb2uLD2OtVgt7tVottP1s/PDwcNhbW1vDnmlpWdy1zenp6aZtZmYmHNtoNMJ+7bXXhn379u1hf+CBB8LO2emiiy4K+6pVq8Le19cX9t7e3rD39PSE/fjx42HP5mx7e3vYly1bFvZsTmfbj+bV/v37w7Esnux6mPnQhz4U9iNHjjRtJ06cCMdm51S270Xvs9mcnp2dDXt2L+rs7Ax7ds2Yn58Pe/b+s/2L5nxHR0c49g//8A/D/s///M9hn5iYCDtn7p3vfGfYs3tNJLtPdHV1hT07Z7P+s5/9LOynTp0Ke7Z/2bN/NqczCwsLYc+e/R999NGmbceOHeHYl770pWH/6le/GnbOTYODg2GP7uGVSn4fPHDgQNjf9KY3hf3GG28Me3Ze/p//83/Cfv/994f96NGjYY/WEiqVSqW7uzvs0fdZ/FIKAAAAgCVgUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdG1LvQOn64orrgj75s2bwz48PNy0tbXFh6G1tTXsXV1dYV+/fn3Ye3p6wt7SEq8dzs/Phz17fwsLC2GvVqthb29vD3utVgv7+Ph42A8ePHjG285k7/3Nb35z2N/1rncVen2WxtDQUNj7+/vD3tvbG/aBgYGwnzhxIuzZNSe7JmT7l+ns7Ax7tn/Lly8v9PosjbVr14Z9zZo1YT916lTT1tHREY7NruXZfTI757M5U6/Xw57dK7KePSdk+59tPzt+2fiJiYmmbWZmJhyb7fvLX/7ysH/+858PO2fuH/7hH8L+jne8o2k7fvx4OPbo0aNhz+6j2bNrZm5uLuzZfT4zNjYW9unp6ULbz2TvL3rOOHDgQDj2q1/96hntE0svupdt3bo1HNvX1xf2yy+/POzZeXXo0KGwb9u2LezZNSF7jsi+b1911VVh37RpU9iz/Y++r1Yq8b0uG/vLwC+lAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0rUt9Q6cruuvvz7snZ2dZ9zr9Xo4trW1Neyzs7Nhf+973xv2Q4cOhf3gwYNhX7duXdgPHz4c9paWeG1ybm4u7Nmx7+vrC/sznvGMsP/bf/tvm7bh4eFwbFtbfIpnn/1rXvOasL/rXe8KO2engYGBsGdzZmFhIeyXXXZZ2JcvXx72mZmZsGeyOZ2ZmpoKe7VaDfull15a6PVZGtl5uWbNmrBH86KjoyMc29vbG/ZarRb27D6UXeuzczrrmew5Itt+0f3PrlmrVq1q2rL7bPbZvuAFLwj75z//+bBz5m6//faw/+hHP2raXvGKV4Rjb7vttrBnz189PT1hHxkZCXv2bJqdt9l9Ntu/7P2NjY2FPZpzpyPavz/+4z8utG3OXlu3bm3aNm7cGI6dnp4O+759+8K+c+fOsGfXm6NHj4Z9y5YtYb/22mvD/uMf/zjsz3rWs8J+4MCBsH/rW98Ke3affe5zn9u0Pfjgg+HY3bt3h/2pwC+lAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0rUt9Q6crte85jVhr9VqYW9tbW3aFhYWwrFdXV1hP3XqVNj/y3/5L2F/4QtfGPZnPOMZYf/kJz8Z9t///d8P+549e8K+YsWKsEfHtlKpVI4ePRr2j3zkI2F/61vf2rS1tcWncPbZTU1NhX379u1hv+iii8K+d+/esLM4Ojs7w97f3x/2bE7Mz88XGj84OBj2DRs2hL23tzfsY2NjYc/O++Hh4bAvX7487GvXrg07Z6edO3eGPbvWr1mzpmlraYn/DyzrMzMzYT906FDYH3744bDv378/7JOTk2HP9i8bn11TOjo6wp59di972cvCHu1/dr3q6+sLe3a9Yun81V/9VdP29re/PRz72GOPhf348eNhz+ZEdp8aHx8Peya7nmX7lz1/tre3hz3b/4GBgbDffPPNTVv2DMC5K7oeHzt27IzHViqVSqPRCPs3v/nNsGfn3ctf/vKwf+Mb3wh79pxwyy23hD37vp9dE1auXBn27JoRXROy5+Z9+/aFfWJiIuznAr+UAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdBalAAAAAChd/PdMzyJPe9rTwn7gwIGwR39GMvvz8Zlly5YVGv/1r3897NmfmLz00kvD/q53vSvsN910U9izP+GZ/VncO++8M+xXXHFF2Gu1WtOW/anp7M9/1uv1sGd/8vg5z3lO2Pfu3Rt2FseKFSvCnv3p1OHh4bAPDQ2FPbumZOdtdl52d3eHfdeuXYW2H825SiX+8/GVSqVSrVbDztnpC1/4Qti///3vh/2Nb3xj07Zjx45w7J//+Z+H/YEHHgh7UT09PWHP5lzWsznf1dUV9uw54POf/3zY/+RP/iTsP/7xj5u21atXh2OnpqbCvnXr1rCzeLLns+haf/XVV4djP/jBD57RPv1cdt5k96Fszk1PT4c9OzZZn52dDXv25+sz2fivfOUrhbbP2Sk7rzs6Opq2bM5k95HsPrhq1aqwZ/exRx99NOzt7e1hv+2228J+6NChsGffl7Pjl83J7Nk3uqZk296wYUPYF/sZqQx+KQUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJSubal34Od27NgR9uPHj4e9VquFvbW1tWmrVqvh2O7u7rCPjIyEPZO999nZ2bCvXbs27B/84AfDnr3/+fn5QuOf85znhD1z6NChpm39+vXh2IWFhbDX6/WwT09Ph/2aa64J+6c+9amwsziWL18e9mxOZedFR0dHoe1H16NKpVK57LLLwv7444+HfdOmTWHfv39/2GdmZsI+NjYW9uyawdnpL//yL8OezYtvf/vbTdtdd90Vjl22bFnYH3jggbBn96HsnM3u46Ojo2HPzvlGoxH2bP8HBgbCnl0zHn744bC/8Y1vbNomJibCsdmxy66HLJ7s2Thy+PDhsGfn1Pnnnx/27D4zPj4e9ux6lG2/pSX+f/nsvF+1alXYs2Ofvf6jjz4adp6ahoaGwh7dK7JzvqurK+wnTpwIe2dnZ9iz78uDg4Nhf/Ob3xz2bP9Wr14d9uw+m92r2triZZNszq9YsaJpm5ubC8dm7y17RjoX+KUUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQural3oGfe+973xv27u7usE9MTIR9YWHhjLc9MzMT9lqtFvYrr7wy7CtXrgz7ihUrwt7e3h721atXh31+fj7s2fvv6OgI++DgYNhf97rXhX358uVN2/T0dDh2YGAg7Nn47L1lny1Lo6urK+xTU1OFtp/Nuf7+/rAPDw+HvdFohH10dDTs2ZzdvHlz2EdGRsKeXfOy48PZ6Rvf+EbYb7jhhrC/+tWvbtpe+MIXhmM/9alPhf0tb3lL2LP7zAUXXBD2vr6+sGdzsrW1NezZvWRubi7s9Xo97J/5zGfCPj4+HvboGSzbt5MnT4b9Va96VdivuuqqsJ84cSLsLI2Wlvj/tbP7YHZOd3Z2hn1sbCzs2ZzL7pPZeZ/J7pOZY8eOFRrPuSk776PvrNl9LHt27O3tDXv0XbpSyedU9uz9ile8Iuzf/e53w75///6wZ88JbW3xskh2n+/p6Qn72rVrm7bdu3eHY9esWRP2pwK/lAIAAACgdBalAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAErXttQ78HO7du0K+5o1a8J+wQUXhH3ZsmVNW29vbzj2oYceCvvCwkLYb7311rDX6/VCPXv91tbWsLe1xadBtVot9PotLfHa5/j4eNj37t3btPX09IRjs/ee7duhQ4fC/r/+1/8KO0sjmzPT09OFtp+dN6dOnQr7JZdcUuj1T548GfaJiYmwZ9e0TZs2hb2zszPs2Zzm7PQXf/EXYZ+fnw97dL28//77w7Evf/nLw/6nf/qnYc9k+z47Oxv27D7XaDTCXqvVwp7dq9rb28Pe19cX9uyacfvttzdtR44cCcd++9vfDnt2vTlx4kTYWTzRvSy7jx48eDDsO3fuPOPXrlTyOZnNuWzOZHO6q6sr7NlzxMzMTNiHhobC/vjjj4c9kj3XZ9cjls7g4GDYo+e7/v7+Mx5bqeRzLpsTme7u7rDfcsstYT9w4EDYs/3L5mQ2fm5uLuzZs/HU1FTTVvTYZ9/Vs+vl2cAvpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNK1LfUO/Nzf/M3fFOrLly8P+4UXXti0veUtbwnHPu95zwv7iRMnwr5nz56wj46Ohr29vT3sra2tYV9s1Wo17C0t8drnzMxM2AcGBpq2u+++Oxz7xje+Mew8NWXnXL1eL7T9bHxnZ2fY+/v7C73+ww8/HPanPe1pYd+7d2/YJycnwx7NyUqlUllYWAg7Z6cvfelLYb/hhhvCfuWVVzZtN998czj2y1/+ctjPO++8sD/22GNhz+6T2X22q6sr7G1txR6narVa2KempsI+NzcX9mXLloV98+bNTdsf/dEfnfHYSqVSue6668J+1113hX337t1hZ2ns378/7Nl9uKOjI+zZc332+tmcWrlyZdhPnjxZaPuzs7Nhz45Ptn3OTT09PWHPng/n5+ebtq1bt4Zjs/tI9n200WiEPZPdZ8fHx8OezZnsPp/17D6ePft3d3eHfWhoqGnLjm123mTXs+Hh4bCfDfxSCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKF3bUu/Ak+XkyZNhv/3225u22dnZcOzzn//8sDcajbB3dHSEvbe3N+ytra1hr9frYc9Uq9VCPXv9zs7OsM/NzYW9q6uradu1a1c4ll9O2TlZq9XCPjU1Ffbp6emwDw0NFdp+Zu/evWG/6qqrwj4zMxP2o0ePhn3dunVhz65ZnJ0uvfTSsGfn/ZEjR5q2W2+9NRz73Oc+N+w7duwIe3YfLnpOZteU7PWL3meLPgdEn02lUql87nOfa9p2794djn3kkUfCfuDAgbBn1zPOTtn1oOizaTY+mxPRs+PpbD/7XpHd5/v7+8OeaW9vLzSes1N2r8jOy+g747Jly8Kx2ffdotra4mWF7L13d3eHPbvmZPr6+sKe3Yfn5+fDftFFF4V9/fr1TVs237PvDatXrw778PBw2M8GfikFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApbMoBQAAAEDpLEoBAAAAUDqLUgAAAACUrm2pd+B0VavVsLe3t4d9bm6uaWs0GuHYsbGxsLe2toZ9YWEh7NnrZ7JjU3T7iy07fpHR0dFFfe16vR72s/3Y8otln1tbW3xpnJmZCXt2PSp63t57772Fxg8NDYU9u6YcP3487ObFuWnr1q1hz+bFhg0bmrYjR46EY6empsJeq9XCPj4+HvaWlvj/4LLtF73PF9Xb2xv2+fn5sK9atSrs0fHv7+8Px0afe6VSqQwODoZ9zZo1YX/kkUfCzpnLnnEi2ZzJ7hPRc3mlUqmcPHnyCe/TExmfvX53d3fYjx07FvZszk1MTISdp6bs+XBycvKMx2fPpiMjI2FfuXJl2Is+O2fPltmcmJ6eDnt2bLP7ZLb/mew+PTw83LRl3wu6urrCnl2vzgV+KQUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJSubal34HQ1Go2wz8/Pn/G2H3744bCPjY2Fva0tPoxzc3NPeJ/+b9l7r1arhcZnsu1nsvff3t5+xtvOPptMS0u8LruwsFBo+yyN7HPt6OgIe1dXV9izc7per4d9YmIi7Jk77rgj7Nn7b21tDXu2/52dnWGfmpoKO2en7LyZmZkJe3S9HB8fD8f29PSEPTsns3M669l9Ljs2Wc+2n72/ote07P0PDw+HPbJixYqwZ89I69atC/sjjzzyhPeJ0xOdV9k52d/fH/bly5eHPbtPZOdVJjuns2vOwMBA2Is+22fXhM2bN5/xtmu12hmPZXFl5112vYy+061cuTIcmz27Ff2+mZ132X0ou89l14TJycmwz87Ohj37bLL9z97/6tWrm7Y1a9aEY0dGRsKefW85F/ilFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApbMoBQAAAEDpLEoBAAAAULq2pd6BJ0tLS7y+trCw0LRNT0+HY+fm5sLe2dkZ9lqtFva2tvhjqFarYW80GoXGZz07ttnrz87Ohr2npyfs0f5lx5ZfTtk5m53z2Zxcvnx5oe3fd999Yc+Mjo4WGp/N2dbW1kXdPmenovOmXq83bSdOnAjHdnd3n/G2K5Xi96lMNr7ofXh+fj7s2XNGds3Kjs+RI0eatpmZmXBs9HxVqeTXk/7+/rCzeLJ5FTl+/HjY9+zZE/YDBw6EPXs2zM7L1atXhz17tt+/f3+h1x8YGAj74cOHw75u3bqwc25auXJl2Ds6OsIeXU8nJyfPaJ9+LvtO1d7eHvbsXpB9H8xk97FsTmb3mmz/s/c/PDwc9ug5p+h727hxY9jPBX4pBQAAAEDpLEoBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlK5tqXfgydJoNM54bL1eD/vCwkKh1856S0uxtcFs/1tbWwttv1qthj3b/+z9Z/sfbb/I5/5kjOfs1NHREfbu7u6wDw8Ph33dunVh7+zsDPuBAwfCnhkfHw97rVYLe1tbfOnP5nS2/bm5ubBzbsruJdG1/OjRo+HYbE4Wld3HsvtQ0TmT9Wz/sueQovf5InM2e2+Lve8sjWuuuSbsjzzySNgfffTRsM/MzIR9bGws7MuWLQv7wMBA2Kenp8OezZm1a9eGPbNmzZqwn3feeU3bsWPHwrHZnM2uh5y59vb2sGfPrxdeeGHTll1Ljxw5EvYdO3aEfWJiIuxdXV1hzxQ972ZnZ8OePbufPHky7M985jPDfurUqbBHz0GrV68Ox2bPCENDQ2E/F/ilFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApbMoBQAAAEDpLEoBAAAAULq2pd6Bc8H69evDfvLkybC3traGvdFohL2lJV47rFarYV9q2f7Pz8+HPXp/2bGFX2RoaCjsU1NTYe/o6Ah7e3t72Pft2xf2osbHx8Oe7f/09HTYu7q6wj45ORl2zk7ZvSgTXauz+2Q2Z7J9q9frYc/uk7VaLezZfSzbv6LHtuj+Ze+/u7u7aRsdHQ3HZteDTNHxNJedF9G82bhxYzj20ksvDfsjjzwS9sHBwbBn9+nsPtrb2xv2888/P+zZeb9s2bKwFzUxMRH2N7zhDU3bRz/60XBsdr1k8WTX8ux7TfT8NjIycsZjK5X4PlCp5Odkpq+vL+xzc3OFxg8MDBTafjbnt2zZEvb77rsv7LfddlvT9uIXvzgce88994Q9u8dv37497A888EDYy+CXUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOnalnoHniyNRmPRtl2r1QqN7+joCPvCwkLYq9Xqovbs2GXj6/V62Nvb28M+Ozsb9mj/sm1nFvO8Yelkc6qnpyfsGzZsCHt23mVz/sEHHwx7USdOnAj74OBg2CcmJsKezRvziieqq6sr7Nl9JrtPtbTE/weXjc8UPeeLzqm5ubmwZ++/u7u7adu3b1849vLLLw97tm9Fjz3NZfMm8uu//uthv++++8KezemxsbGwb9myJeyPP/542Ldv3x727NgcPHgw7Dt37gz70aNHw75y5cqwnzx5Muzr169v2i644IJwbDanWTzZvMieX6Px3//+98Ox2Tk/NTUV9tbW1rBnsu/T2f61tRVbtpicnAx79mxcdN6MjIycUatU8vto9owwNDQU9rOBX0oBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAACla1vqHTgXzM7Ohr21tTXstVqt0Ph6vR72RqNRaPtzc3OFtt/WFp9G2fipqamwRwYHB894LDTT29tbaHy1Wg37yZMnC20/c/DgwbBfcsklYc+uee3t7WHPrimcncbHx8OezYuWljP/f67u7u6wF71PZffRTLb9bM5nPbtPZ68/Pz9f6PWjz+6xxx4Lx1555ZVhL/oMxdLYuXNn2O++++6wZ59rR0dH2Ds7O8OeKXpeZdeMrM/MzIR948aNYR8bGzvjvmXLlnDsvn37ws7iya7VExMTYY/Oq+z7ZpF79OnI5uzo6GjYs2PT1dUV9lOnToV9w4YNYc+OzyOPPBL2devWhf348eNNW/Z8lV0vDxw4EPbsvDob+KUUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQural3oFzQb1eX9TtV6vVsDcajULbb2mJ1x6z189k+1f0/dVqtaatu7s7HJspemw5N2Vzoqenp1Cfm5sL+8mTJ8Ne1LFjx8K+ffv2sA8ODhbqjz/+eNhZGh0dHWHProfZvBkbG3vC+/Rz7e3tYZ+fnz/jbVcq+XvLjs3CwkLYi95H29rix7Hs9bPnlOz9R6+/f//+cGz22WX7no1n8WzZsqVpO3z4cDi2q6sr7BMTE2HPzvno2a9SKf78l20/m1OdnZ2FXn9qairsq1evDnt0n121atUZ7ROLL7tXtLa2hj26z2Zzrre3N+xF73PZnMrmfNazOZkdu2z7o6OjYc/m/HnnnRf26Dnj9ttvD8dmn9309HTYs3PjbOCXUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAACli/82IpVKJf8z2EVlf6q5qGz/i/4p66J/RjwbH/2J0Z6ennAs/CLZn63NztnsvBsfHw/73Nxc2IsaGRkp9PrZ8Yn+rO3pjGdpZNfarGd/Tjn6E+WZ7E85Z/uW/anoTHYfLNqz/cv+FHfR45Ntv7+/v2nbu3dvOLbon/Eu+gzCmdu0aVPTln1u2eee3Se6urrCnp2z2etnli9fHvbF/vP2P/vZz8J+4YUXhv3o0aNN28DAQDh2xYoVYT9x4kTYOXNFz6toXg0PD4djr7zyyrAXNTs7G/bsPlb02Ti6j1UqlcrMzEzYe3t7C73+xMRE2Ddu3Ni0ZffZa6+9NuzZsR8cHAz72cAvpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNK1LfUOPFkajcaSvXZra+uibj97b9VqtdD2i+5/0WPf0hKvjS4sLDRti33sOTd1dXWFfXJyMuzZnOrp6Qn7oUOHwr7Y9u/fH/b29vawz8zMFHr9+fn5QuNZGtm1PLtWP/7442f82tm2s33Lzuls+9m9pF6vhz2TXVOy9xfdB09n+5mBgYGm7d577w3HZsc260X3nTMXnffZ5zY1NRX27D6Zzdm5ubmwZ3Mym1N9fX1hr9VqYZ+dnQ37+vXrw37HHXeE/dprrw374cOHm7a2tvjr3fLly8N+4sSJsLN0ijyfTU9Phz2bk9l5lc2Z7D6W9Wz/smfP7JrU29sb9lOnToV9bGws7NH+j46OhmOzZ5Tself0ub4MfikFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApbMoBQAAAEDpLEoBAAAAUDqLUgAAAACUrm2pd+DJUq1Ww95oNM5423Nzc2Hv6ek5422fjnq9HvbW1taw12q1sC/msXsyLCwsNG3Ze88s9XtjcbS3t4c9mxMdHR2F+ujoaNgX27Fjx8KenfdZz45vds3i7JR97i0t8f9jPfbYY2f82rOzs2E/fvx42MfHx8OezflMdB+qVPL7aHbssvFZ7+zsDHtXV1fYe3t7m7bHH388HJvtW3Y9aGt7yjyKnnOGhoaatuw+l83JHTt2hD07J8fGxsKe7V825/v7+wttf2ZmJuw7d+4M+1e/+tWwZ88R0f4tX748HGvOnb2ya3l0n122bFk49rLLLgv73XffHfZszmbfybLzLhs/Pz8f9uw5oru7u9D47Dkg27/o+BV9RsnGnwtz3i+lAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0rUt9Q48FbS0xGt7CwsLYa9Wq4W2X7TX6/WwZ/uXaTQaYc/2L9La2nrGY3nqam9vD3utViu0/eycnZ6eLrT9bM5lc2pmZibsc3NzYc+uWWNjY4Ven6WRnVdFr/XZeRHp7Ows1Ofn58O+YsWKsGfnfHbNKHrsij4HZMe+t7c37OvWrWvasvnc0dER9ra2+FEzG8/iGRoaatqyc25kZCTsAwMDYc/Oi8OHD4c9O29OnjwZ9snJybAXeTY9HRMTE2HP9j96ds/e29q1a8P+4IMPhp0zt2zZsrBv3Lgx7Lt3727aNm3aFI7dsmVL2H/605+GPZuz2X0y+86W3YcPHToU9pUrVxbafjZvsmva1NRU2M8777ymLXuuz55xomt5pZK/97OBX0oBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAACla1vqHXiyNBqNRdv2oUOHwn7RRReFvVarhb1erxfq7e3ti7r97NguLCyEva2t2GkWvX5ra+uibZunrpMnTxYaPzk5Gfbp6elC229pif+/IJtzw8PDYS96TcrmzczMTNhZGtn1cm5uLuzZeZOdt5EvfvGLYV+2bFnYjx07FvbsPpS9t0y2/Wq1WqhnczLb/1OnToX9jjvuCHuR117M84Zi+vr6mrapqalw7PLlywu9dldXV9iz61E251atWhX248ePh723t7fQ9oeGhsK+bdu2sGdzPpo32dj+/v6ws3j27NkT9p/97Gdhj67lK1euDMf+7//9v8Pe3d0d9kzR++js7GyhPjg4GPbx8fGwZ3M+e/bOnv2jz6ejoyMce9NNN4U9m9Pz8/NhPxt4EgAAAACgdBalAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAErXttQ7cC4YHBwMe29vb9jb2uLDPDQ0FPaWlnjtMOvt7e1hL2phYSHsra2tYT9w4EDYe3p6mrZt27aFYzPZsavX64W2z9JYtWpVoT4yMhL2rq6usM/MzIQ9k52X2Zyr1Wph7+zsDHuj0Qh7R0dH2Pv6+sLO0uju7g57tVoNe3ZeZvfKyIc+9KEzHsvSyq4Xi3neUMyFF17YtP3sZz8Lx2b3wUx2XkTPfpVKfp/dtWtX2N/whjeEPXt2v+WWW8Je9Nk9mxeTk5NNW/bZffvb3w47i2dsbKxQjzzjGc8447GVSvFn1+z7cCZ7tu3v7w979p0t27+i17Ts2Te6pmzatCkcu2/fvrCPj4+H/Vzgl1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApbMoBQAAAEDp2pZ6B54s1Wo17I1G44y3fdddd4X9vvvuC/vo6GjY29vbn+gu/f9paYnXFicmJsKeHZvs2NZqtbDX6/Wwz83NhX358uVN2+233x6OzWT7xrnp7rvvDvtXvvKVsGdz8sSJE2H/9re/HfZM0fPyyJEjYX/ooYfCHs25SqVSOXbsWNj37NkTdpZGdt7u3bs37AcPHgz7bbfd9oT36eey+0ymyD2eYj772c+GfevWrWG/8847n8zd4Ql461vf2rRlz3bZs+d//+//Pezbtm0L+6OPPhr2DRs2hH3//v1hv+OOO8Je1Be/+MVC4//xH//xSdoTnkra2pp/dZ+ZmQnHZr2rq6vQ+Ow+nF1Tovd2Oq+fbf+8884Le/Zs29fXF/axsbGwT09Pn/HYTHY9Phe+7/qlFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApbMoBQAAAEDpLEoBAAAAULpqo9FoLPVOAAAAAPDLxS+lAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdBalAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdBalAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdBalAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKF3b6f7DarW6mPsB/AKNRuOMxy72nG1pide06/X6oo6PdHR0hH3Tpk1hv+yyy8J+2223hf3IkSNhX2qbN28O+6WXXhr2r3/962Evct5mFvO8eTKczXMW+JfMWTi3mLNwbjmdOeuXUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOmqjUajcVr/sFpd7H0B/h+nOT1/ocWes9n2s16v18/4tf/u7/4u7J2dnWGfnZ0N++rVq8Pe398f9uxz6+joCPtdd90V9u7u7rDPz8+H/bLLLgv7+Ph42B955JGwDw4ONm1f/vKXw7Ff/OIXw55paYn/r6XIeXc6zuY5C/xL5iycW8xZOLeczpz1SykAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB01Uaj0Titf1itLva+0ER27Fta4rXFer0e9tM8BZoqem4Uff0irrrqqrDv2rUr7BdffHHY9+7dG/bsvRc5Nos9Z4ued5kPfehDTdu2bdvCsYcOHQp7R0dH2BcWFsI+MDAQ9rVr14b9S1/6Utj/9m//Nuw/+tGPwn706NGwT05Ohn14eDjsra2tYY/OjRUrVoRjb7311rB/5CMfCXu2b9lnW9TZPGeBf8mchXOLOQvnltOZs34pBQAAAEDpLEoBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlK5tqXeAxddoNM7p7Ueuu+66sP/Kr/xK2C+88MKw//mf/3nYq9Vq2F/4wheGfXZ2Nuxns5aWeE27Xq+HfevWrWHfsWNH0/bYY4+FYzs7O8OenbPZvj/++OOFXn/z5s1hf+1rXxv2qampsB8/fjzs4+PjYW9tbQ17dnwWFhaatkOHDoVjo8+9Usn3LXrtJ2M8wM9lzwBL+Xy01Ioem6Lji97HFnv/FnP8L/N5x9lrsa+X/f39Yb/66qvDfvPNNxd6/ez9RdekWq1W6LWLyvY982Rcc/xSCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKF3bUu/AU0G1Wg17o9EotP1s/MLCQqHtZ377t3877LfeemvYr7nmmrC/7W1vC/uhQ4eatp07d4ZjH3roobDfeeedYf+jP/qjsO/evTvsT2W1Wq3Q+BtuuCHs9Xq9aevt7Q3HzszMhL2trdilr6+vL+yHDx8O+9DQUNhf/vKXh/2uu+4Ke39/f9i7u7vDHh37SqVSmZ+fD3tLS/P/78iulx0dHWHPriff+c53wp69PsDpKvp8t2PHjrBn99nsXnTHHXc84X16siz2s29msZ+Nl/r9FR0PZYueDSuVfM5ecMEFYX/zm98c9unp6bBPTk6GPftucfvtt4e9yPem7Nk1O7bZ+KLf6VpbWwuNr1T8UgoAAACAJWBRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0bUu9Ayy+7du3h72tLT4NrrvuurBfeeWVYV++fHnY/+Ef/iHs3/ve95q2O++8Mxx7xRVXhP2Zz3xm2Ofm5sJ+wQUXhH3fvn1h/2V26aWXhr1arTZtvb294djsc4u2XalUKo1GI+z1ej3s7e3tYZ+dnQ375ORk2Ds6OgptP9u/hYWFsM/MzIR9YGCgaevq6grHZsd+x44dYf/Od74T9lqtFnaA09XT0xP23/zN3wz7K17xirDffffdYc/uRddcc03YDxw40LQNDg6GY/v7+8OePf8MDQ2FfXh4OOyZbP+z+2R2bFtbW8Oevb/R0dFC28/2P5LdZ7NnhKx3dnaGPTs2n/zkJ8POuSk7p7Nnz+c///lh/7Vf+7WwHzx4MOzZeZtd71/wgheE/e///u+btqNHj4ZjszmbHbtMX19f2LPr4dTUVKHXr1T8UgoAAACAJWBRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0bUu9A08FjUZjUbff09MT9quuuirsR44cCfvY2FjY/+t//a9hf8c73hH2Q4cOhf0jH/lI2M8777ymLTv2Dz74YNivuOKKsL/gBS8I+8zMTNj37dsX9l9m27ZtC3utVmva2tvbw7Hd3d1hzz63+fn5sC8sLIS9Wq2GvbW1tdDrd3R0hD3bv+jYnk7v7OwMe71eb9qyzyY7dqtWrQo7QFle/vKXh/3yyy8P+/ve976wX3PNNWF/0YteFPbsXrd79+6m7fzzzw/HZvepZz/72WEfHh4O+5o1a8K+cuXKsE9PT4f9+PHjYb/44ovDfuLEiULb37lzZ9iz/R8dHW3aZmdnw7HXXntt2LNjG503lUqlcv/994e9r68v7BdeeGHYOTfNzc0VGv/MZz4z7Fu2bAl79uzd0hL/Vucb3/hG2J/+9KeH/S//8i+btjvuuCMce88994Q9m3PPetazwp4d2127doX9Rz/6UdhPh19KAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJSubal34Kkg+xOT0Z9Hr1QqlUajEfbsT6dmf/J3x44dYb/uuuvC/vu///thz/4kcfYnNDPHjh0747HnnXde2LM/6bt+/fqw/+7v/m7Yf/jDH4Z9z549YT+Xtbe3h31iYiLs/f39TVv2p6izz+3AgQNhz+ZU9mdjs2tCprOzs9D4jo6OsGfXpKKi/V+xYkU4Nvtstm7dekb7BPBke/zxx8Neq9XCfuWVV4Y9+zPdp06dKtSf97znNW3f/e53w7Hr1q0L+5ve9Kawf/3rXw979ufds/vYF77whbBnz4e9vb1hX7lyZdi7u7vDfskll4Q9+xPrIyMjTdtFF10Ujl2+fHnYs2essbGxsGfH9uqrrw77Jz/5ybBzdqpWq2HPvu++4AUvCHt2vRwfHw97NqezeZP1H//4x2Hft29f05Z913/Oc54T9le96lVhz+Z0tu9vfvObwz47Oxv20+GXUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOnalnoHngrq9XrYG41Goe1PT0+HvaUlXlt8/vOfH/bPfOYzYf+DP/iDsJ/NVq5cGfZly5aF/Y477gj77Oxs2Ds7O8Oe7d+5bO3atWHv6ekJezRv+vr6wrErVqwI+4MPPhj2bE5lvbW1NezZNSMbn11TqtVq2DPZ/mXn/TOe8YymbXJyMhzb3t4e9sHBwbDz1FT0nM7mTNE5m22/rS1+3KrVamEvKrtmZe9vMWVzPjs2RZ+xiti+fXvYN2zYEPZNmzaFfc+ePWHftm1b2Lds2RL2nTt3Nm3f/va3w7HZPf7hhx8O+9DQUNize8Wjjz4a9szc3FzYDxw4EPZLLrkk7Nlnnz0DZY4ePdq0vfzlLz/jsZVKft5dcMEFYb/yyivDnj17d3d3h53FUfQ+W9QHPvCBsGfXnEw257J7TXbNuPrqq8MezYvsHnznnXeGfd++fWHP3tsf/uEfhn3r1q1hf81rXhP20+GXUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOnalnoHngoajcaibn98fDzs3/ve9wr1THd3d9hnZmbCXvT4VKvVM9722rVrw37ixImwZ8f+5ptvDvu6devCvnnz5rCfy57xjGeEvb29PezR556dk62trWGv1Wphz/atXq8X6ot9zci2n+1fdOwrlUplYWEh7NHxHRgYCMceOXIk7CMjI2HfsmVL2Pfv3x92zk6LPWeyc77o62fXnKLe8pa3hP1973tf2NevX/9k7s4TMj8/v2SvXVR2PVq1alXYs+vdtm3bwt7SEv/fcvb6R48ebdq2bt0ajn3lK18Z9p/85Cdh37BhQ9jvvvvusD//+c8P+/nnnx/2PXv2hP2Zz3xm2Hft2hX25z3veWEfHR0Ne/YMFd2Hs/Miu09m5032DJa9t2z/smcwFsdi32czJ0+eDHv2nW56ejrsnZ2dYW9ri5dF+vr6wp59H47mTfZcfs0114T9qquuCns2584777ywf/3rXw/7k8EvpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNK1LfUOUFxra2vY6/V62Ftaiq1NZuMXFhYKbb+IVatWhX1iYiLs1Wo17Nmx7+vrC3utVgv7uWz16tVhz47t7Oxs07ZmzZpw7NjYWNjb29vDPj8/H/bsc8/eWzZnGo1G2LM5lY3P3l+2/9nxiz67rVu3hmP37t0b9mzfLr/88rDv378/7JybsvMimxOLfS3+V//qX4X96U9/ethf+9rXhn16ejrsw8PDYf/85z/ftGX7XlRHR0fY3/Oe94T9P/yH//Bk7s4Tkt3jf/azn4X9Bz/4Qdhf9KIXhb27uzvsDzzwQNije2V2n/3Yxz4W9uuvvz7s2fPZDTfcEPbs2GV9/fr1Yf/a174W9p07d4b9kksuCfsXvvCFsH/9618P+5YtW5q2u+++Oxz77Gc/O+wrVqwIe+a+++4Le3ZeHj16tNDrc27q6ekJe/bsnPWpqamwnzp1KuwjIyNhj+ZkpRI/hxT93pAdu+x7Q7ZWsHHjxrA/GfxSCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKF3bUu8AxS0sLCzq+Onp6bC3trYWev1qtRr2RqNxxtvu7e0N++/8zu+E/Z/+6Z/C/rnPfS7sExMTYZ+amgr7uWzbtm1hb29vD/vMzEzTtnLlynDs3r17w16v18Oe7VumpSVe78/O6WxOZXMmk73/7PWz8zoan207OzbZsb344ovDztmp6H2gyH2iUqlULrjggrC/9rWvDftVV10V9he+8IVhf/jhh8N+8ODBsI+NjYV9y5YtYX/JS14S9sX0+te/Puy/+qu/WtKePHGrV68O+4kTJ8J++eWXh33ZsmVhn5+fLzQ+2v+nPe1p4dhbbrkl7LVaLezZtfqd73xn2LPnp9/6rd8K+4YNG8L+yU9+Muzf/e53w3799deH/cEHHwx7d3d32F/zmtc0bYODg+HYhx56KOydnZ1hX79+fdizfb/vvvvC3t/fH3YWR3Yfzp6/su+TfX19YV+3bl3YZ2dnC/XsvJ6bmwt7ds3J5t3IyEjT1tPTE47t6OgI+/j4eNgHBgbCfvfdd4c9++yuvPLKsJ8Ov5QCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABK17bUO3A2qFarYW80GiXtyblpYWEh7K2trYu6/cjw8HDY77rrrrBfeeWVYf+7v/u7sG/bti3su3btCvu5bO3atWHv6uoK++joaNPW09MTjp2ZmQl7W1t86Ss651taiq33Z9ekWq1WaPuZ2dnZsHd0dIT95MmTTVt7e3s4Njt2vb29Yc/Ou19m2bGt1+thzz73ubm5J7xPP1d0zg0ODob9gx/8YNhf97rXhX1qairshw8fDvvtt98e9mxedHd3h/2BBx4I+4YNG8L+gQ98IOyR8847L+zZsf1P/+k/hX379u1hv+KKK8L+k5/8JOxFZNv+jd/4jbDv27cv7Nl59bznPS/sq1atCvvHPvaxpm316tXh2Pe85z1hz+4j7373u8N+9OjRsL/97W8P+8qVK8M+Pz8f9uc85zlh//KXvxz2j3/842G/7rrrwr5mzZqw//SnP23aHnzwwXDsy172srBv2rQp7Hv27Al7dj172tOeFvYf/ehHYWdxZPfh7Ptc9n0tuxdk5/zx48fDnt0ns2ec7Ply48aNYc+egTo7O5u27HqUfW/J3nt2Pfzrv/7rsF9++eVhz/bvdPilFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApbMoBQAAAEDpLEoBAAAAULq2pd6Bs0Gj0VjqXXhKW1hYWLRtX3755WH/6U9/GvYvfOELYX/Zy14W9l//9V8Pe0dHR9gPHDgQ9nPZypUrw97e3n7G267VamGfnp4+421XKpVKS0u8Xl+0Z6rVatizY1ev18M+MzMT9uy8za6ZExMTYY9k723ZsmVhX7du3Rm/9rkuO2+ynpmbmys0PnLDDTeE/dWvfnXY3/CGN4R9ZGQk7Pfdd1/Ys2tOdl5m18PsmjU1NRX2K6+8MuxHjhwJe3T83v3ud4djs32/5557wt7Z2Rn2rq6usI+Pj4d9MU1OTob9xS9+cdjvvffesH/+858Pe3ZerVixIuzRM0g2p7JzftOmTWG/7bbbwv7www+H/dOf/nTYX/WqV4U9u0/feeedYd+6dWvYs/N6+fLlYc/u49Fnf9ddd4Vjs/Mi27ebb7457P/6X//rsHd3d4e96L2KM9PWFi8LFH0G2LNnT9hnZ2fDnj0ftra2hj37PnreeeeFPXt2zp4zov3P7nO9vb1hP3nyZNgPHjwY9ux6/+EPfzjst956a9hPh19KAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApWtb6h3g3Nfa2hr2hYWFQtt/73vf27StWLEiHPs3f/M3YX/Tm94U9pGRkbB/7WtfC/vmzZvDPjc3F/ZzWXd3d9ir1WrYu7q6mrahoaFw7OTkZNizc7aoer0e9paW+P8Dsv2bmZl5wvv0RLaf7X97e3vYp6ammrbsnI8+90qlUuno6Ah7dmyfyhqNRtiLXoszb3vb25q2P/iDPwjHrl69OuwHDx4M+z333BP27L1nr5/J5kz22WTnbbb948ePh33ZsmVhj+zatSvsN9544xlvu1KpVN73vveF/a1vfWvYH3vssbD/1m/91hPep9N18cUXh/3OO+8Me3ZeXnrppWH//ve/H/a2tvgx/7nPfW7Tdvfdd4djx8bGwn7JJZeEPfvc3vjGN4Y9O/b/9E//FPbe3t6wX3311WGfn58P++7du8M+PT0d9mxOR/fZl770peHYvXv3hv2jH/1o2C+66KKwZ+dddj3buHFj2JdS9uyaPV9l1/ps+9F5lx3XTK1WKzQ+k31nyp7dszmTPR9m9+FszmWfbfb8ml0ziozNPvts33fu3Bn2U6dOhf3J8Mv79A4AAADAkrEoBQAAAEDpLEoBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFC6tqXeAc59CwsLYd+yZUvY3//+94e9tbW1aTt+/Hg49jWveU3YH3roobC3tcVTZN26dWGfn58P+7mss7Oz0Piurq6wr1q1qmnbvXt3OHZ0dDTsq1evDvvs7GzY6/V62KNztlKpVFpa4v8PyM6b7LzMTE9PF9p+9tkfPXq0aZucnAzHDg0Nhb1arYY9ux61t7eH/Wyes894xjPC/oIXvCDsF198cdizOZld7/r6+pq2bE4+/vjjYR8YGAh7tu9ZbzQaYZ+amgp7dl4VPW+zOZldk7I5PzMz07Q961nPCsceOnQo7NF5UalUKgcPHgx7dp/u6ekJ++/93u+FvYhs37q7u8N+5MiRsD/44INhf9Ob3hT2++67L+z3339/0/a+970vHPujH/0o7GvWrAn7S17ykrBHzwCVSqWyadOmsGfnXXTOVyqVyhve8Iawf/nLXw579pyxcePGsI+Pj4d97dq1TVu2b9k9/MYbbwz7bbfdFvaf/OQnYX/lK18Z9r1794Z9MWXPb9m1ularPZm7U6prr7027K9+9avD/tznPjfs2X10ZGQk7B0dHWHP7pPZZ5ftX3ZuZPMqeg4p+gySyY7dxMRE2F/1qleF/Stf+coT3qf/l19KAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApWtb6h04Xa2trWFfWFgoaU/OPtmxqVarYe/o6Aj71NRU2Ldv3x72D3/4w2F/6KGHwr5x48am7Z3vfGc4ttFohD1z+eWXh33r1q1h/9GPflTo9c9my5cvLzS+pSVeE+/v72/a6vV6OLatrdilLbueZOdVNueyXlS2/9mxz64Js7OzYe/t7W3aJicnw7EXXXRR2Hfv3h32bN/PO++8sD/++ONhX0z/5t/8m7C/6lWvCnt3d3fYs/Nubm4u7O3t7WGP7hXZa/f19YU9m/PZeTU6Ohr27JqRvX5XV1fYs/ff2dkZ9uw+n3322f5Fn+3Y2Fg4tlarhf3kyZOFxmfvLbpXLLbsvPn+978f9uxzv/7668N+xRVXhP3QoUNhn5mZadoeeeSRcOzFF18c9kx2H/3Wt74V9uxzX7VqVdiz+9iePXvCfvvtt4c9O2+zz77INeHAgQPh2AsvvDDsN954Y9izY/ulL30p7F/5ylcKbX8xLfb3yRUrVoR93bp1YY8+u2xs9gyRPX9lcyZ7tsy+T65cuTLsRa5nlUrx58PsGamnpyfsu3btatqyZ6Brr7027NkzyqlTp8I+Pz8f9mc/+9lhfzL4pRQAAAAApbMoBQAAAEDpLEoBAAAAUDqLUgAAAACUzqIUAAAAAKWzKAUAAABA6SxKAQAAAFC6tqXegdO1sLBQaHy1Wj3jsY1Go9BrL7bs2LS2toZ9amoq7OvXrw/7O9/5zrB/61vfCvuzn/3ssL/2ta8N+2LKPvuix/ZcNjg4GPbOzs6wt7TEa+K9vb1N26OPPhqO7erqCnutVgt79rnW6/WwZ+dN9t6Ljs/2L5O9fvbZzs7ONm333ntvOHbTpk1hn5ubC3v22UXn1VL79Kc/HfYf//jHYb/qqqvCvmPHjrBv3rw57P39/WFfvnx509bWFj9uZPex7JxftWpVoZ7Nmey86ujoCHv2/rP3l5mYmAj75ORk2KN5lV0vs/c+MzNTaHy279H1plKpVL761a+G/T3veU/YI+vWrQv72NhY2LPr0ejoaNj37NlT6PXf9KY3NW2rV68Ox46MjIR9eno67Nn1KjvvbrvttrA/9NBDYe/u7g77xz/+8bBfccUVYV+5cmXYd+/eHfbsmrVly5am7fnPf3449uabbw77T37yk7Bnz3/Z9fLAgQNhL/KdrajsO8kHPvCBsGefW3bsinyny64X2ZwaHx8Pe/b8lX1u2TVh165dYf/N3/zNsN9xxx1hz55hsntJNOdOx6/8yq80bdm+ZXMm+76ZXe/6+vrCnj0fPhn8UgoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdBalAAAAAChd21LvQFkajcZS78IZq1arYc/e28LCQqHXf//73x/2Q4cOhf1pT3ta2F/3utc90V0qTXbshoaGwj43N/dk7s5ZpaOjI+zz8/Nh7+3tDXtnZ2fT9vWvfz0cm51z2b61tBRbr29riy+tXV1dYc/Om2z7ra2tYa/X62HP9i87ftFn99BDD4VjX/va14a9r68v7Nmx6+npCftSyq71e/bsCfttt91W6PWjz61SqVTOP//8sF9wwQVN25YtW8Kx69atC3t2TmbHLpvT2ZwYHh4O+8TERNhHRkbCPjo6uqh9eno67FNTU2GPZPeC7LPJZMd+cnIy7Iv5/Dc2Nhb29evXh33t2rVhv+OOO8KePX9t27Yt7IcPH27a9u/fH47N5vTs7GzYv/Od74Q9O68efPDBsK9YsSLsJ06cCPvq1avD3t7eHvZszm/evLnQ+KNHjzZtg4OD4djnPve5Yc+O7de+9rWwX3zxxWFfuXJl2KPzsqjs+eiv/uqvwp7N2ex7Q9YX81qcvXZ2n8gMDAyEPTvn/+Iv/iLs2f695S1vCXt2vZyZmQn7LbfcEvZHHnkk7BdeeGHTls2J7Nk2ux5lz0DZc/3x48fD/mTwSykAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0bUu9A6erWq2GvdFohH1wcLBpW716dTh27dq1Yf/Od74T9qKy91bUn/3Zn4W9VquFfefOnWG/8cYbn/A+na62tmKncPbesu0PDQ0Vev1zWXbsMtmcjrY/NzcXjs0+txMnToS9pSVer6/X64Vef3JyMuwLCwth7+zsLNQzw8PDYc+uSRs3bmzafvCDH4RjT506Ffb29vawT0xMhH1gYCDsS2l0dDTsvb29Yc/uVdmcy2TzJroXdnV1hWPn5+fPZJf+f1pbW8OenbPZnM/2P3v9jo6OsGfXjOz1+/r6wr5q1aqwL1u2rGnL5lz22WXvraenJ+zj4+OFXv/RRx8NexHZvSD73J7znOeE/cILLwx7dt5Gz76VSqVy0003NW379+8Px1511VVh37NnT9jvueeesGf3sd/7vd8Le/acMDIyEvbsevuNb3wj7HfccUfY3/ve94Z9x44dYf/EJz7RtP30pz8Nx/7Jn/xJ2NetWxf26HpRqVQqGzZsCPtDDz0U9sW8T//2b/922Ddv3hz2hx9+OOzZtTjrK1asCHsku1Znx/XAgQNhP3ToUNiza/nRo0fD/qlPfSrsv/EbvxH2r3zlK2HfsmVL2LPP5oorrgj79ddfH/boep1dr7LrYfaMkcm+d2TnVvTcf7r8UgoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdBalAAAAAChd21LvwOlqNBqFxl966aVN28aNG8OxY2NjYe/p6Qn71NRU2Bfb+vXrw37VVVeFvaurK+zXXHPNE96nJ0t2XtTr9UXd/qZNmwpt/1yWnffz8/Nhn5mZCXtnZ+cZj+3o6Aj7mjVrwn7ixImwd3d3h33lypVhP3bsWNiXL18e9uzYjo+Phz3bv+y8Hh0dDXtvb2/Tls2p7LO55557wp6dG9lndzabnJws1IvKjl17e3vTtrCwEI7t6+sLe3Q9yF77dLS2toa9pSX+P7xarbaor5/J5vyhQ4fCXq1Wm7a2tvhRMTv22bHJtp+Nz56xsvdexNGjR8M+PT0d9vvvvz/s2Xk/ODgY9q997Wth/853vtO0Pf3pTw/H3nrrrWF/+OGHw549Q2Tvff/+/WFfvXp12KP71Om8/qpVq8K+Y8eOsO/ZsyfsIyMjYY/uldn14JFHHgl7dj1atmxZ2LNn74mJibAPDw+HvYjs+evAgQNh7+/vD/vs7Gyh7Wf3wuj5NvtcsmfbRx99NOzZvmXXu+z5LLvW33TTTWHPng+3bNkS9hUrVoR9bm4u7NmzcfTsnr33bE5l9+FsfPQMUKnk36suuuiisJ8Ov5QCAAAAoHQWpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKF38d3j/L9mfCsz+zHdRRV9/165dT+bunFM+8YlPhD37M44vfelLn8zdeVJlf2Y8O2+Kbn/79u2Ftn8uy/486MqVK8M+MDAQ9ujYZ3+SN7seZH/qOfvTrNGfda1U8j/Nmv0p6ey8yv4Ud/Ynj5cvXx72lpb4/yuKHP8jR46EYw8fPhz2Bx54IOwXXnhh2LPzluayP/ec9cjJkyfPeCwslYsvvjjsr3/968N+6NChsI+Pj4f9+PHjYX/DG94Q9m3btjVt2Z9XP//888O+YcOGsH/zm98M+9Of/vSwZ88YExMTYc9k98kLLrgg7CMjI2HfsWNH2LP9j7Z/+eWXh2N37twZ9rGxsbD39vaGPXt2zu7Tz3nOc8JexOOPPx727Pnx4MGDYc+OzdDQUNhHR0fDPjw83LRl14O2tvhrf/ZsnD3bdnV1hT17dsyePaP3XqlUKpdccknYJycnw37gwIGwZ88p2fGL9j/7XlH0e0l3d3fY16xZE/ZTp06FPbvmnA6/lAIAAACgdBalAAAAACidRSkAAAAASmdRCgAAAIDSWZQCAAAAoHQWpQAAAAAonUUpAAAAAErXdrr/sNFoLOZ+LPrrV6vVpu1rX/taOHb9+vVh/9CHPhT2z3/+82Ev6k//9E/D/qIXvSjsH/vYx8K+Z8+eJ7xPTxVtbfEUWb58eUl7cvbp6+sr1DPt7e1N26/+6q+GY48fPx72jRs3hn1ubi7svb29YV9YWAh7a2tr2IeHh8M+MTER9uzYZ+f1iRMnwn7ZZZeFfXR0tGl7wQteEI7t7OwMezbnZmdnw7569eqwA5yu8fHxsH/zm98Me1dXV9h37NgR9ux6d9ttt53x+J6ennBsf39/2Gu1WtivuOKKsEf3kUolvw9nsvvovffeG/bsPrp27donvE//t+xetWXLlqYte8Z47LHHwr5ixYqwZ+fd/v37C/UHHngg7EXs3r077F/60pfC/ru/+7thP3ToUNgfeeSRsM/MzIQ9er6LnpsrlUqlu7s77B0dHWHPzqvsvMiejbPv+lNTU2E/fPhwoe1n+5fN+SKfXfa9I7seZn1+fj7s2fX6/PPPD/vRo0fDfjr8UgoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdBalAAAAAChd2+n+w+uuuy7sc3NzYR8bGwv7yZMnwz45ORn22dnZsM/MzJxRq1QqlW3btoX9ne98Z9hvueWWsB87dizsL3zhC8P+tre9Lezf/e53w/7Hf/zHYT+XNRqNQuNbWuJ12+zceSpbtWpV2Pft2xf2gYGBsPf39zdtR44cCcd2dXWFPbtedHd3h71Wq4W9Wq2GPdu/iYmJsC8sLIS9vb097Nn+nzp1Kux9fX1hj45vR0dHODa71m/fvj3s2Xsrek0A+Llly5aF/fjx42Fva4sfw2+44Yaw33XXXWG//fbbwz48PNy0XX311eHY7Lm+p6cn7MuXLw/7TTfdFPYrrrgi7Js2bQp7vV4P+6FDh8Kevf/s9bPnkOw+Pjo62rRl9+gHH3ww7NkzwIte9KKwZ997sueA888/P+yL6UMf+lDYd+/eHfZ3vetdYd+yZUvYozlZqcSfe/b81NraGvbsc8muV9n2s2fj7PksmxNZz95fNj7b/0w0/ujRo+HYbE6vWLEi7Nn1bs2aNWG/++67w/6Zz3wm7J/+9KfDXqn4pRQAAAAAS8CiFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApbMoBQAAAEDp2k73H27ZsqVQX7VqVdiXLVsW9vn5+bCfOHEi7PV6vWk7cOBAOPazn/1s2O++++6w33DDDWG/6qqrwr5z586w//CHPwz7O9/5zrDPzc2FvbOzM+yzs7NhP5dNTU2F/Zvf/GZJe3L26ejoKNSz866rq6tpazQa4djsc8vO6ZmZmbAXNTg4GPaf/exnhbZfrVbDnh2f1tbWsB87dizs0eczMTERjh0fHw/7wsJC2LPrUa1WCzvA6br33nvD3tPTE/bsevY//+f/DHt2rb700kvDfvjw4abtyJEj4djs2fdlL3tZ2I8fPx721atXh31sbCzs99xzT9iz7w3t7e1h7+7uDvvjjz8e9ujYVyr5+4/OndHR0XDshg0bwp7d4++///6wr1+/Puznn39+2P/H//gfYS+ipSX+PUb0fbFSqVRuvvnmQv36668P+4c+9KGwb968uWkbGBgIx2bvPbuetLXFywbZ9SxT5NmyUsnnXPZ8mD2fZscnE+1/ts6RPbdnn+0///M/hz2b07t27Qr7k8EvpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJVG41G47T+YbW62PsSWrlyZdg3bNgQ9hUrVpzx2Oy9b968OeyXXHJJ2Pv7+8P+gx/8IOyf+9znwn7gwIGw09yWLVvCfuedd4Y9Ou9Ox2lOz19osefs6173urC/613vCvv+/fvDfsEFFzRt+/btC8f29fWFfXJyMuy1Wi3sCwsLYe/t7Q376tWrwz42Nhb2I0eOhL2o7u7usC9fvjzs0Xn76KOPhmOf9axnhT07NsPDw2H/6Ec/Gvbvfve7Yc+czXMW+JfMWTi3mLO/2Pbt28M+NDQU9tHR0bBn35ez5/r5+fmwP/zww2Hn3HU6c9YvpQAAAAAonUUpAAAAAEpnUQoAAACA0lmUAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNK1LfUOnK6RkZFCHc7E/v37w/7Xf/3X5ezIWejee+8N+9TUVNh37twZ9n//7/9901ar1cKxK1euDPvw8HDYu7u7w37hhReG/RWveEXYs/OqXq+H/aKLLgr7iRMnwt7e3h72b37zm2FvaYn/P2NgYKBpy459NLZSqVSuuOKKsI+Ojob9hz/8YdgBAM41DzzwwKJuf8+ePYu6fX65+aUUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQumqj0Wic1j+sVhd7X4D/x2lOz19oqefsi1/84rBfffXVYf+zP/uzpm1ubu6M9omz38DAQNg/9rGPhf0HP/hB2P/+7//+Ce/TE3Euz1n4ZWTOwrnFnIVzy+nMWb+UAgAAAKB0FqUAAAAAKJ1FKQAAAABKZ1EKAAAAgNJZlAIAAACgdBalAAAAACidRSkAAAAASldtNBqNpd4JAAAAAH65+KUUAAAAAKWzKAUAAABA6SxKAQAAAFA6i1IAAAAAlM6iFAAAAAClsygFAAAAQOksSgEAAABQOotSAAAAAJTOohQAAAAApfv/AB9u9iBhPnX2AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from keras.datasets import fashion_mnist\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\n",
        "\n",
        "unique_labels=np.unique(y_train)\n",
        "images=list()\n",
        "for label in unique_labels:\n",
        "  images.append(x_train[np.where(y_train==label)[0][0]])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(len(images)):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(images[i], cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMa7db27WCEC"
      },
      "source": [
        "Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "k8lZoXzGM85H"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNeuralNetwork:\n",
        "\n",
        "  def __init__ (self,x_train,y_train,epochs,hls,neurons_in_hl,eta,activation):\n",
        "    self.epochs=epochs\n",
        "    self.hls=hls\n",
        "    self.neurons_in_hl=neurons_in_hl\n",
        "    self.eta=eta\n",
        "    self.activation=activation\n",
        "\n",
        "    self.x_train=self.input_flattening_train(x_train)\n",
        "    self.y_train=self.one_hot_encoding_train(y_train)\n",
        "\n",
        "\n",
        "    #initialize W\n",
        "    W=np.empty((self.hls+1,), dtype=object)\n",
        "    w_input=np.random.rand(self.neurons_in_hl,self.x_train.shape[1]).astype(np.float128)\n",
        "    W[0]=w_input\n",
        "    # W[0] = np.random.randn(self.neurons_in_hl, self.x_train.shape[1]) / np.sqrt(self.x_train.shape[1]).astype(np.float128)\n",
        "    for i in range(self.hls-1):\n",
        "      w_hls=np.random.rand(self.neurons_in_hl,self.neurons_in_hl).astype(np.float128)\n",
        "      W[i+1]=w_hls\n",
        "      # W[i + 1] = np.random.randn(self.neurons_in_hl, self.neurons_in_hl) / np.sqrt(self.neurons_in_hl).astype(np.float128)\n",
        "    w_output=np.random.rand(self.y_train.shape[1],self.neurons_in_hl).astype(np.float128)\n",
        "    W[self.hls]=w_output\n",
        "    # W[self.hls] = np.random.randn(self.y_train.shape[1], self.neurons_in_hl) / np.sqrt(self.neurons_in_hl).astype(np.float128)\n",
        "\n",
        "    #initialize B\n",
        "    B=np.empty((self.hls+1,), dtype=object)\n",
        "    for i in range(self.hls):\n",
        "      b_hls=np.random.rand(self.neurons_in_hl).astype(np.float128)\n",
        "      B[i]=b_hls\n",
        "      # B[i] = np.random.randn(self.neurons_in_hl) / np.sqrt(self.neurons_in_hl).astype(np.float128)\n",
        "    b_output=np.random.rand(self.y_train.shape[1]).astype(np.float128)\n",
        "    B[self.hls]=b_output\n",
        "    # B[self.hls] = np.random.randn(self.y_train.shape[1]) / np.sqrt(self.neurons_in_hl).astype(np.float128)\n",
        "\n",
        "    self.W=W\n",
        "    self.B=B\n",
        "\n",
        "\n",
        "  def one_hot_encoding_train(self,y_train):\n",
        "    temp=list()\n",
        "    for i in range(y_train.shape[0]):\n",
        "      vector=np.zeros(10)\n",
        "      vector[y_train[i]]=1\n",
        "      temp.append(vector)\n",
        "    return np.array(temp)\n",
        "    # return to_categorical(y_train)\n",
        "\n",
        "  def one_hot_encoding_test(self,y_test):\n",
        "    temp=list()\n",
        "    for i in range(y_test.shape[0]):\n",
        "      vector=np.zeros(10)\n",
        "      vector[y_test[i]]=1\n",
        "      temp.append(vector)\n",
        "    return np.array(temp)\n",
        "    # return to_categorical(y_test)\n",
        "\n",
        "  def input_flattening_train(self,x_train):\n",
        "    return x_train.reshape(x_train.shape[0],-1)/255.0#2048.0\n",
        "\n",
        "  def input_flattening_test(self,x_test):\n",
        "    return x_test.reshape(x_test.shape[0],-1)/255.0#2048.0\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "  def grad_sigmoid(self,x):\n",
        "    return self.sigmoid(x)*(1-self.sigmoid(x))\n",
        "\n",
        "  def tanh(self,x):\n",
        "    return math.tanh(x)\n",
        "\n",
        "  def grad_tanh(self,x):\n",
        "    return 1-((self.tanh(x))**2)\n",
        "\n",
        "  def relu(self,x):\n",
        "    return max(0,x)\n",
        "\n",
        "  def grad_relu(self,x):\n",
        "    if x<0:\n",
        "      return 0\n",
        "    return 1\n",
        "\n",
        "  def softmax(self,a):\n",
        "    max_a=np.max(a)\n",
        "    exp_a=np.exp(a-max_a)\n",
        "    sum_exp=np.sum(exp_a)\n",
        "    return exp_a/sum_exp\n",
        "    # e=np.exp(a)\n",
        "    # return e/np.sum(e)\n",
        "\n",
        "  def update_parameters(self,eta,del_w,del_b):\n",
        "    for i in range(del_w.shape[0]):\n",
        "      self.W[i]-=eta*del_w[i]\n",
        "    for i in range(del_b.shape[0]):\n",
        "      self.B[i]-=eta*del_b[i]\n",
        "\n",
        "  def incerement_grad(self,a,x):\n",
        "    for i in range(a.shape[0]):\n",
        "      a[i]+=x[i]\n",
        "    return a\n",
        "\n",
        "\n",
        "  def forward_propagation(self,w,b,x):\n",
        "    if(self.activation==1):\n",
        "      vectorized_activation_func=np.vectorize(self.sigmoid)\n",
        "    elif(self.activation==2):\n",
        "      vectorized_activation_func=np.vectorize(self.tanh)\n",
        "    else:\n",
        "      vectorized_activation_func=+np.vectorize(self.relu)\n",
        "\n",
        "    a=np.empty((self.hls+2,), dtype=object)\n",
        "    h=np.empty((self.hls+2,), dtype=object)\n",
        "\n",
        "    h[0]=x\n",
        "\n",
        "    for k in range(1,self.hls+1):\n",
        "      temp_prod=np.dot(w[k-1],h[k-1])\n",
        "      # temp_prod=w[k-1] @ h[k-1]\n",
        "      a[k]=b[k-1]+temp_prod\n",
        "      h[k]=vectorized_activation_func(a[k])\n",
        "\n",
        "    temp_prod=np.dot(w[self.hls],h[self.hls])\n",
        "    a[self.hls+1]=b[self.hls]+temp_prod\n",
        "    y_cap=self.softmax(a[self.hls+1])\n",
        "\n",
        "    return a,h,y_cap\n",
        "\n",
        "  def backward_propagation(self,h,a,y,y_cap):\n",
        "    if(self.activation==1):\n",
        "      vectorized_grad_func=np.vectorize(self.grad_sigmoid)\n",
        "    elif(self.activation==2):\n",
        "      vectorized_grad_func=np.vectorize(self.grad_tanh)\n",
        "    else:\n",
        "      vectorized_grad_func=np.vectorize(self.grad_relu)\n",
        "\n",
        "    del_a=np.empty((self.hls+2,), dtype=object)\n",
        "    del_w=np.empty((self.hls+1,), dtype=object)\n",
        "    del_b=np.empty((self.hls+1,), dtype=object)\n",
        "    del_h=np.empty((self.hls+2,), dtype=object)\n",
        "\n",
        "    del_a[self.hls+1]=-(y-y_cap)\n",
        "    for k in range(self.hls+1,0,-1):\n",
        "      del_w[k-1]=np.outer(del_a[k],h[k-1])\n",
        "      del_b[k-1]=del_a[k]\n",
        "      del_h[k-1]=np.dot(self.W[k-1].T,del_a[k])\n",
        "      max_gradient=1.0\n",
        "      del_h[k - 1]=np.clip(del_h[k - 1],-max_gradient,max_gradient)\n",
        "      if k>1:\n",
        "        del_a[k-1]=np.multiply(del_h[k-1],vectorized_grad_func(a[k-1]))\n",
        "\n",
        "    return (del_w,del_b)\n",
        "\n",
        "  def gradient_descent(self):\n",
        "    for iter in range(self.epochs):\n",
        "      del_W,del_B=np.zeros((self.hls+1,), dtype=object),np.zeros((self.hls+1,), dtype=object)\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_W=self.incerement_grad(del_W,ret[0])\n",
        "        del_B=self.incerement_grad(del_B,ret[1])\n",
        "        # print()\n",
        "        # break\n",
        "      self.update_parameters(self.eta,del_W,del_B)\n",
        "    return self.W,self.B\n",
        "\n",
        "  def stochastic_gradient_descent(self):\n",
        "    for iter in range(self.epochs):\n",
        "      del_W,del_B=np.zeros((self.hls+1,), dtype=object),np.zeros((self.hls+1,), dtype=object)\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_W=self.incerement_grad(del_W,ret[0])\n",
        "        del_B=self.incerement_grad(del_B,ret[1])\n",
        "        # print()\n",
        "        # break\n",
        "      self.update_parameters(self.eta,del_W,del_B)\n",
        "    return self.W,self.B\n",
        "\n",
        "  # def mini\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cPEKJgg66791",
        "outputId": "4625d948-6eaa-45e3-96b7-c51379919371"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'FeedForwardNeuralNetwork' object has no attribute 'one_hot_encoding_test'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-943346d178e4>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print(w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my_test1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx_test1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_flattening_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# print(x_test1.shape[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FeedForwardNeuralNetwork' object has no attribute 'one_hot_encoding_test'"
          ]
        }
      ],
      "source": [
        "epochs=2\n",
        "hls=3\n",
        "neurons_in_hl=32\n",
        "eta=10**(-3)\n",
        "activation=1    #1-sigmoid, 2-tanh, 3-ReLU\n",
        "\n",
        "\n",
        "Model=FeedForwardNeuralNetwork(x_train,y_train,epochs,hls,neurons_in_hl,eta,activation)\n",
        "w,b=Model.gradient_descent()\n",
        "# print(w)\n",
        "\n",
        "y_test1=Model.one_hot_encoding_test(y_test)\n",
        "x_test1=Model.input_flattening_test(x_test)\n",
        "# print(x_test1.shape[0])\n",
        "\n",
        "accuracy=0\n",
        "for i in range(x_test1.shape[0]):\n",
        "  a,h,y_pred=Model.forward_propagation(w,b,x_test1[i])\n",
        "  print(x_test1[i])\n",
        "  print(y_pred)\n",
        "  print()\n",
        "  if(np.argmax(y_test1[i])==np.argmax(y_pred)):\n",
        "    accuracy+=1\n",
        "    # print(i)\n",
        "  # print(y_pred)\n",
        "\n",
        "print(accuracy)\n",
        "print(accuracy/y_test1.shape[0])\n",
        "\n",
        "#print(Model.W)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNeuralNetwork:\n",
        "\n",
        "  epochs=0\n",
        "  hls=0\n",
        "  neurons_in_hl=0\n",
        "  eta=0.0\n",
        "  activation=0\n",
        "  num_of_layers=0\n",
        "  x_train=np.zeros(784)\n",
        "  y_train=np.zeros(10)\n",
        "  W=dict()\n",
        "  B=dict()\n",
        "\n",
        "  def __init__ (self,x_train1,y_train1,epochs,hls,neurons_in_hl,eta,activation):\n",
        "    self.epochs=epochs\n",
        "    self.hls=hls\n",
        "    self.neurons_in_hl=neurons_in_hl\n",
        "    self.eta=eta\n",
        "    self.activation=activation\n",
        "    self.num_of_layers=hls+2\n",
        "\n",
        "    self.x_train=self.input_flattening(x_train1)\n",
        "    self.y_train=self.one_hot_encoding(y_train1)\n",
        "\n",
        "\n",
        "    #initialize W\n",
        "    w=dict()\n",
        "    w[1]=np.random.randn(self.neurons_in_hl,self.x_train.shape[1])\n",
        "    # W[0] = np.random.randn(self.neurons_in_hl, self.x_train.shape[1]) / np.sqrt(self.x_train.shape[1]).astype(np.float128)\n",
        "    for i in range(2,self.num_of_layers-1):\n",
        "      w[i]=np.random.randn(self.neurons_in_hl,self.neurons_in_hl)\n",
        "      # W[i + 1] = np.random.randn(self.neurons_in_hl, self.neurons_in_hl) / np.sqrt(self.neurons_in_hl).astype(np.float128)\n",
        "    w[self.num_of_layers-1]=np.random.randn(self.y_train.shape[1],self.neurons_in_hl)\n",
        "\n",
        "    # W[self.hls] = np.random.randn(self.y_train.shape[1], self.neurons_in_hl) / np.sqrt(self.neurons_in_hl).astype(np.float128)\n",
        "\n",
        "    #initialize B\n",
        "    b=dict()\n",
        "    for i in range(1,self.num_of_layers-1):\n",
        "      b[i]=np.random.randn(self.neurons_in_hl)\n",
        "      # B[i] = np.random.randn(self.neurons_in_hl) / np.sqrt(self.neurons_in_hl).astype(np.float128)\n",
        "    b[self.num_of_layers-1]=np.random.randn(self.y_train.shape[1])\n",
        "    # B[self.hls] = np.random.randn(self.y_train.shape[1]) / np.sqrt(self.neurons_in_hl).astype(np.float128)\n",
        "\n",
        "    self.W=w\n",
        "    self.B=b\n",
        "\n",
        "\n",
        "  def one_hot_encoding(self,y):\n",
        "    temp=list()\n",
        "    for i in range(y.shape[0]):\n",
        "      vector=np.zeros(10)\n",
        "      vector[y[i]]=1\n",
        "      temp.append(vector)\n",
        "    return np.array(temp)\n",
        "    # return to_categorical(y_train)\n",
        "\n",
        "  # def one_hot_encoding_test(self,y_test):\n",
        "  #   temp=list()\n",
        "  #   for i in range(y_test.shape[0]):\n",
        "  #     vector=np.zeros(10)\n",
        "  #     vector[y_test[i]]=1\n",
        "  #     temp.append(vector)\n",
        "  #   return np.array(temp)\n",
        "    # return to_categorical(y_test)\n",
        "\n",
        "  def input_flattening(self,x):\n",
        "    return x.reshape(x.shape[0],-1)/255.0#2048.0\n",
        "\n",
        "  # def input_flattening_test(self,x_test):\n",
        "  #   return x_test.reshape(x_test.shape[0],-1)/255.0#2048.0\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    clipped_x=np.clip(x,-500,500)\n",
        "    return 1/(1+np.exp(-clipped_x))\n",
        "\n",
        "  def grad_sigmoid(self,x):\n",
        "    clipped_x=np.clip(x,-500,500)\n",
        "    s=1/(1+np.exp(-clipped_x))\n",
        "    return s*(1-s)\n",
        "\n",
        "  def tanh(self,x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "  def grad_tanh(self,x):\n",
        "    return 1-(np.tanh(x)**2)\n",
        "\n",
        "  def relu(self,x):\n",
        "    return np.maximum(x,0)\n",
        "\n",
        "  def grad_relu(self,x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "  def softmax(self,a):\n",
        "    # return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "    # max_a=np.max(a)\n",
        "    # exp_a=np.exp(a-max_a)\n",
        "    # sum_exp=np.sum(exp_a)\n",
        "    # return exp_a/sum_exp\n",
        "    # e=np.exp(a)\n",
        "    # return e/np.sum(e)\n",
        "    return np.exp(a)/np.sum(np.exp(a),axis=0)\n",
        "\n",
        "  def update_parameters(self,eta,del_w,del_b):\n",
        "    for i in range(1,len(del_w)):\n",
        "      self.W[i]-=eta*del_w[i]\n",
        "    for i in range(1,len(del_b)):\n",
        "      self.B[i]-=eta*del_b[i]\n",
        "\n",
        "  def update_parameters_mgd(self,del_w,del_b):\n",
        "    for i in range(1,len(del_w)):\n",
        "      self.W[i]-=del_w[i]\n",
        "    for i in range(1,len(del_b)):\n",
        "      self.B[i]-=del_b[i]\n",
        "\n",
        "  def incerement_grad(self,a,x):\n",
        "    for i in range(1,len(a)):\n",
        "      a[i]+=x[i]\n",
        "    return a\n",
        "\n",
        "  def initGrads(self):\n",
        "    del_w=dict()\n",
        "    del_w[1]=np.zeros((self.neurons_in_hl,self.x_train.shape[1]), dtype=np.float128)\n",
        "    for i in range(2,self.num_of_layers-1):\n",
        "      del_w[i]=np.zeros((self.neurons_in_hl,self.neurons_in_hl), dtype=np.float128)\n",
        "    del_w[self.num_of_layers-1]=np.zeros((self.y_train.shape[1],self.neurons_in_hl), dtype=np.float128)\n",
        "\n",
        "    del_b=dict()\n",
        "    for i in range(1,self.num_of_layers-1):\n",
        "      del_b[i]=np.zeros(self.neurons_in_hl, dtype=np.float128)\n",
        "    del_b[self.num_of_layers-1]=np.zeros(self.y_train.shape[1], dtype=np.float128)\n",
        "\n",
        "    return del_w,del_b\n",
        "\n",
        "  #def initHistory(self):\n",
        "\n",
        "\n",
        "  def forward_propagation(self,w,b,x):\n",
        "    # if(self.activation==1):\n",
        "    #   vectorized_activation_func=np.vectorize(self.sigmoid)\n",
        "    # elif(self.activation==2):\n",
        "    #   vectorized_activation_func=np.vectorize(self.tanh)\n",
        "    # else:\n",
        "    #   vectorized_activation_func=+np.vectorize(self.relu)\n",
        "\n",
        "\n",
        "    h=dict()\n",
        "    a=dict()\n",
        "\n",
        "    h[0]=x\n",
        "\n",
        "    for k in range(1,self.num_of_layers-1):\n",
        "      # temp_prod=np.dot(self.W[k],h[k-1])\n",
        "      # temp_prod=w[k-1] @ h[k-1]\n",
        "      a[k]=b[k]+np.dot(w[k],h[k-1])\n",
        "      if(self.activation==1):\n",
        "        h[k]=self.sigmoid(a[k])\n",
        "      elif(self.activation==2):\n",
        "        h[k]=self.tanh(a[k])\n",
        "      else:\n",
        "        h[k]=self.relu(a[k])\n",
        "\n",
        "    # temp_prod=np.dot(self.W[self.num_of_layers-1],h[self.num_of_layers-2])\n",
        "    a[self.num_of_layers-1]=b[self.num_of_layers-1]+np.dot(w[self.num_of_layers-1],h[self.num_of_layers-2])\n",
        "    y_cap=self.softmax(a[self.hls+1])\n",
        "\n",
        "    return a,h,y_cap\n",
        "\n",
        "  def backward_propagation(self,h,a,y,y_cap):\n",
        "    # if(self.activation==1):\n",
        "    #   vectorized_grad_func=np.vectorize(self.grad_sigmoid)\n",
        "    # elif(self.activation==2):\n",
        "    #   vectorized_grad_func=np.vectorize(self.grad_tanh)\n",
        "    # else:\n",
        "    #   vectorized_grad_func=np.vectorize(self.grad_relu)\n",
        "\n",
        "    # del_a=np.empty((self.hls+2,), dtype=object)\n",
        "    # del_w=np.empty((self.hls+1,), dtype=object)\n",
        "    # del_b=np.empty((self.hls+1,), dtype=object)\n",
        "    # del_h=np.empty((self.hls+2,), dtype=object)\n",
        "    del_a=dict()\n",
        "    del_w=dict()\n",
        "    del_b=dict()\n",
        "    del_h=dict()\n",
        "\n",
        "    del_a[self.num_of_layers-1]=-(y-y_cap)\n",
        "    for k in range(self.num_of_layers-1,0,-1):\n",
        "      del_w[k]=np.outer(del_a[k],h[k-1])\n",
        "      del_b[k]=del_a[k]\n",
        "      del_h[k-1]=np.dot(self.W[k].T,del_a[k])\n",
        "      # max_gradient=1.0\n",
        "      # del_h[k - 1]=np.clip(del_h[k - 1],-max_gradient,max_gradient)\n",
        "      if k>1:\n",
        "        if(self.activation==1):\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],self.grad_sigmoid(a[k-1]))\n",
        "        elif(self.activation==2):\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],self.grad_tanh(a[k-1]))\n",
        "        else:\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],self.grad_relu(a[k-1]))\n",
        "\n",
        "    return (del_w,del_b)\n",
        "\n",
        "  def gradient_descent(self):\n",
        "    for iter in range(self.epochs):\n",
        "      # del_W,del_B=np.zeros((self.hls+1,), dtype=object),np.zeros((self.hls+1,), dtype=object)\n",
        "      del_w,del_b=self.initGrads()\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "      self.update_parameters(self.eta,del_w,del_b)\n",
        "    return self.W,self.B\n",
        "\n",
        "  def stochastic_gradient_descent(self,batch_size):\n",
        "    for iter in range(self.epochs):\n",
        "      # del_W,del_B=np.zeros((self.hls+1,), dtype=object),np.zeros((self.hls+1,), dtype=object)\n",
        "      del_w,del_b=self.initGrads()\n",
        "      num_of_points_seen=0\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          self.update_parameters(self.eta,del_w,del_b)\n",
        "          del_w,del_b=self.initGrads()\n",
        "    return self.W,self.B\n",
        "\n",
        "  def momentum_gradient_descent(self,beta,batch_size):\n",
        "    prev_uw,prev_ub=self.initGrads()\n",
        "    for iter in range(self.epochs):\n",
        "      del_w,del_b=self.initGrads()\n",
        "      num_of_points_seen=0\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          uw,ub=dict(),dict()\n",
        "          for i in range(1,len(del_w)):\n",
        "            uw[i]=beta*prev_uw[i]+self.eta*del_w[i]\n",
        "            ub[i]=beta*prev_ub[i]+self.eta*del_b[i]\n",
        "\n",
        "          self.update_parameters_mgd(uw,ub)\n",
        "          prev_uw=uw\n",
        "          prev_ub=ub\n",
        "\n",
        "    return self.W,self.B\n",
        "\n",
        "  def nestrov_gradient_descent(self,beta,batch_size):\n",
        "    prev_uw,prev_ub=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      del_w,del_b=self.initGrads()\n",
        "      uw,ub=dict(),dict()\n",
        "      for i in range(1,len(del_w)):\n",
        "        uw[i]=beta*prev_uw[i]\n",
        "        ub[i]=beta*prev_ub[i]\n",
        "\n",
        "      self.update_parameters_mgd(uw,ub)\n",
        "      num_of_points_seen=0\n",
        "\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          for i in range(1,len(del_w)):\n",
        "            uw[i]=beta*prev_uw[i]+self.eta*del_w[i]\n",
        "            ub[i]=beta*prev_ub[i]+self.eta*del_b[i]\n",
        "\n",
        "          self.update_parameters_mgd(uw,ub)\n",
        "          prev_uw=uw\n",
        "          prev_ub=ub\n",
        "\n",
        "    return self.W,self.B\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xBOcPhBapvwQ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=2\n",
        "hls=3\n",
        "neurons_in_hl=32\n",
        "eta=10**(-3)\n",
        "activation=1    #1-sigmoid, 2-tanh, 3-ReLU\n",
        "\n",
        "\n",
        "Model=FeedForwardNeuralNetwork(x_train,y_train,epochs,hls,neurons_in_hl,eta,activation)\n",
        "w,b=Model.nestrov_gradient_descent(0.9,25)\n",
        "# print(w)\n",
        "\n",
        "y_test1=Model.one_hot_encoding(y_test)\n",
        "x_test1=Model.input_flattening(x_test)\n",
        "# print(x_test1.shape[0])\n",
        "\n",
        "accuracy=0\n",
        "for i in range(x_test1.shape[0]):\n",
        "  a,h,y_pred=Model.forward_propagation(w,b,x_test1[i])\n",
        "  # print(x_test1[i])\n",
        "  # print(y_pred)\n",
        "  # print()\n",
        "  if(np.argmax(y_test1[i])==np.argmax(y_pred)):\n",
        "    accuracy+=1\n",
        "    # print(i)\n",
        "  # print(y_pred)\n",
        "\n",
        "print(accuracy)\n",
        "print(accuracy/y_test1.shape[0])\n",
        "\n",
        "#print(Model.W)"
      ],
      "metadata": {
        "id": "RZUDDyLkp0rS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939e34bb-b75a-47a6-b113-b3be09b9ae18"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "0.1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}