{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iDZfsf-15KJ",
        "outputId": "46a6ca4d-0b66-4ec0-9ef6-aee9343bc7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.41.0-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grssrur62HfI",
        "outputId": "bfc61d84-f0c0-4b2c-9980-42ee880faecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1"
      ],
      "metadata": {
        "id": "RhYKbNTutVed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from keras.datasets import fashion_mnist\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\n",
        "output_class=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "wandb.init(project=\"Pritam CS6910 - Assignment 1\",name=\"Question 1\")\n",
        "\n",
        "img,getPlot=plt.subplots(2,5,figsize=(20,6))\n",
        "getPlot=getPlot.flatten()\n",
        "output_images=[]\n",
        "for i in range(10):\n",
        "  imgClass=np.argmax(y_train==i)\n",
        "  getPlot[i].imshow(x_train[imgClass],cmap=\"gray\")\n",
        "  getPlot[i].set_title(output_class[i])\n",
        "  img=wandb.Image(x_train[imgClass],caption=[output_class[i]])\n",
        "  output_images.append(img)\n",
        "wandb.log({\"Question 1\":output_images})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Smv6RNQu2Ovr",
        "outputId": "f4e959fc-8618-414d-cf9e-13dcaf6892ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7439fcf415f7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# from keras.utils import to_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__internal__/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_initialize_variables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minitialize_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrack_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtLarge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtSmall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_preprocessing_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Generic layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m  \u001b[0;31m# pyright: ignore # noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mArrowDtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m from pandas.core.groupby import (\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mGrouper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.core.groupby.generic import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_series_with_explicit_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m from pandas.core.groupby.groupby import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0msanitize_masked_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m )\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_key_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m from pandas.core.indexes.api import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_docs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_shared_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_indexer_indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m from pandas.core.window import (\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mExpanding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mExponentialMovingWindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.core.window.ewm import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mExponentialMovingWindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mExponentialMovingWindowGroupby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m from pandas.core.window.expanding import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/window/ewm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcommon\u001b[0m  \u001b[0;31m# noqa: PDF018\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m from pandas.core.indexers.objects import (\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mBaseIndexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mExponentialMovingWindowIndexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2"
      ],
      "metadata": {
        "id": "eIc-FbA1Bemx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from keras.datasets import fashion_mnist\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\n",
        "validation_ratio=0.1\n",
        "num_validation_samples=int(validation_ratio*x_train.shape[0])\n",
        "\n",
        "validation_indices=np.random.choice(x_train.shape[0],num_validation_samples,replace=False)\n",
        "\n",
        "x_val,y_val=x_train[validation_indices],y_train[validation_indices]\n",
        "x_train, y_train=np.delete(x_train,validation_indices,axis=0),np.delete(y_train,validation_indices,axis=0)"
      ],
      "metadata": {
        "id": "f4QbEDVPtZom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2320560d-1fdf-4fe5-bd92-7e9913505760"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation:\n",
        "  def sigmoid(x):\n",
        "    clipped_x=np.clip(x,-500,500)\n",
        "    return 1/(1+np.exp(-clipped_x))\n",
        "\n",
        "  def grad_sigmoid(x):\n",
        "    clipped_x=np.clip(x,-500,500)\n",
        "    s=1/(1+np.exp(-clipped_x))\n",
        "    return s*(1-s)\n",
        "\n",
        "  def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "  def grad_tanh(x):\n",
        "    return 1-(np.tanh(x)**2)\n",
        "\n",
        "  def relu(x):\n",
        "    return np.maximum(x,0)\n",
        "\n",
        "  def grad_relu(x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "  def softmax(a):\n",
        "    max_a=np.max(a)\n",
        "    exp_a=np.exp(a-max_a)\n",
        "    sum_exp=np.sum(exp_a)\n",
        "    return exp_a/sum_exp\n",
        "    # clipped_a=np.clip(a,-500,500)\n",
        "    # return np.exp(clipped_a)/np.sum(np.exp(clipped_a),axis=0)"
      ],
      "metadata": {
        "id": "70dfB8nrBeNQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpdateParameters:\n",
        "  def update_parameters(W,B,eta,del_w,del_b):\n",
        "    for i in range(1,len(del_w)):\n",
        "      W[i]-=eta*del_w[i]\n",
        "    for i in range(1,len(del_b)):\n",
        "      B[i]-=eta*del_b[i]\n",
        "    return W,B\n",
        "\n",
        "  def update_parameters_mgd(W,B,del_w,del_b):\n",
        "    for i in range(1,len(del_w)):\n",
        "      W[i]-=del_w[i]\n",
        "    for i in range(1,len(del_b)):\n",
        "      B[i]-=del_b[i]\n",
        "    return W,B\n",
        "\n",
        "  def update_parameters_rms(W,B,eta,vw,vb,del_w,del_b,eps):\n",
        "    for i in range(1,len(vw)):\n",
        "      updated_eta=eta/(np.sqrt(np.sum(vw[i]))+eps)\n",
        "      W[i]-=updated_eta*del_w[i]\n",
        "    for i in range(1,len(vb)):\n",
        "      updated_eta=eta/(np.sqrt(np.sum(vb[i]))+eps)\n",
        "      B[i]-=updated_eta*del_b[i]\n",
        "    return W,B\n",
        "\n",
        "  def update_parameters_adam(W,B,eta,mw_hat,mb_hat,vw_hat,vb_hat,eps):\n",
        "    for i in range(1,len(vw_hat)):\n",
        "      # updated_eta=eta/(np.sqrt(vw_hat[i]+eps))\n",
        "      W[i]-=eta*mw_hat[i]/(np.sqrt(vw_hat[i]+eps))\n",
        "    for i in range(1,len(vb_hat)):\n",
        "      # updated_eta=eta/(np.sqrt(vb_hat[i]+eps))\n",
        "      B[i]-=eta*mb_hat[i]/(np.sqrt(vb_hat[i]+eps))\n",
        "    return W,B\n",
        "\n",
        "  def update_parameters_nadam(W,B,eta,mw_hat,mb_hat,vw_hat,vb_hat,beta1,beta2,del_w,del_b,eps):\n",
        "    for i in range(1,len(vw_hat)):\n",
        "      W[i]-=(eta/(np.sqrt(vw_hat[i]+eps)))*((beta1*mw_hat[i])+(((1-beta1)*del_w[i])/(1-beta1**(i+1))))\n",
        "    for i in range(1,len(vb_hat)):\n",
        "      B[i]-=(eta/(np.sqrt(vb_hat[i]+eps)))*((beta1*mb_hat[i])+(((1-beta1)*del_b[i])/(1-beta1**(i+1))))\n",
        "    return W,B\n",
        "\n",
        "  def normalizeParameters(W,B):\n",
        "    for i in range(1,len(W)):\n",
        "      W[i]=W[i]/np.linalg.norm(W[i])\n",
        "    for i in range(1,len(B)):\n",
        "      B[i]=B[i]/np.linalg.norm(B[i])\n",
        "    return W,B\n"
      ],
      "metadata": {
        "id": "ioOP_wfStxHv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParamaterInitialization:\n",
        "  def initializeW(initialization,num_of_layers,neurons_in_hl,x_train_shape,y_train_shape):\n",
        "    w=dict()\n",
        "    if(initialization==\"random\"):\n",
        "      w[1]=np.random.randn(neurons_in_hl,x_train_shape).astype(np.float128)\n",
        "      for i in range(2,num_of_layers-1):\n",
        "        w[i]=np.random.randn(neurons_in_hl,neurons_in_hl).astype(np.float128)\n",
        "      w[num_of_layers-1]=np.random.randn(y_train_shape,neurons_in_hl).astype(np.float128)\n",
        "\n",
        "    else:\n",
        "      w[1]=(np.random.randn(neurons_in_hl,x_train_shape)/np.sqrt(x_train_shape)).astype(np.float128)\n",
        "      for i in range(2,num_of_layers-1):\n",
        "        w[i]=(np.random.randn(neurons_in_hl, neurons_in_hl)/np.sqrt(neurons_in_hl)).astype(np.float128)\n",
        "      w[num_of_layers-1]=(np.random.randn(y_train_shape,neurons_in_hl)/np.sqrt(neurons_in_hl)).astype(np.float128)\n",
        "\n",
        "    return w\n",
        "\n",
        "  def initializeB(initialization,num_of_layers,neurons_in_hl,x_train_shape,y_train_shape):\n",
        "    b=dict()\n",
        "    if(initialization==\"random\"):\n",
        "      for i in range(1,num_of_layers-1):\n",
        "        b[i]=np.random.randn(neurons_in_hl).astype(np.float128)\n",
        "      b[num_of_layers-1]=np.random.randn(y_train_shape).astype(np.float128)\n",
        "\n",
        "    else:\n",
        "      for i in range(1,num_of_layers-1):\n",
        "        b[i]=(np.random.randn(neurons_in_hl)/np.sqrt(neurons_in_hl)).astype(np.float128)\n",
        "      b[num_of_layers-1]=(np.random.randn(y_train_shape)/np.sqrt(neurons_in_hl)).astype(np.float128)\n",
        "\n",
        "    return b"
      ],
      "metadata": {
        "id": "AMOyi74u2dN7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNeuralNetwork:\n",
        "\n",
        "  epochs=0\n",
        "  hls=0\n",
        "  neurons_in_hl=0\n",
        "  eta=0.0\n",
        "  activation=0\n",
        "  num_of_layers=0\n",
        "  x_train=np.zeros(784)\n",
        "  y_train=np.zeros(10)\n",
        "  W=dict()\n",
        "  B=dict()\n",
        "  bestConfigRun=False\n",
        "\n",
        "  def __init__ (self,x_train1,y_train1,x_val1,y_val1,hls,neurons_in_hl,activation,initialization,epochs,eta,wd):\n",
        "    self.epochs=epochs\n",
        "    self.hls=hls\n",
        "    self.neurons_in_hl=neurons_in_hl\n",
        "    self.eta=eta\n",
        "    self.activation=activation\n",
        "    self.num_of_layers=hls+2\n",
        "    self.initialization=initialization\n",
        "    self.wd=wd\n",
        "\n",
        "    self.x_train=self.input_flattening(x_train1)\n",
        "    self.y_train=self.one_hot_encoding(y_train1)\n",
        "\n",
        "    self.x_val=self.input_flattening(x_val1)\n",
        "    self.y_val=self.one_hot_encoding(y_val1)\n",
        "\n",
        "    self.W=ParamaterInitialization.initializeW(self.initialization,self.num_of_layers,self.neurons_in_hl,self.x_train.shape[1],self.y_train.shape[1])\n",
        "    self.B=ParamaterInitialization.initializeB(self.initialization,self.num_of_layers,self.neurons_in_hl,self.x_train.shape[1],self.y_train.shape[1])\n",
        "\n",
        "\n",
        "  def one_hot_encoding(self,y):\n",
        "    temp=list()\n",
        "    for i in range(y.shape[0]):\n",
        "      vector=np.zeros(10)\n",
        "      vector[y[i]]=1\n",
        "      temp.append(vector)\n",
        "    return np.array(temp)\n",
        "\n",
        "  def input_flattening(self,x):\n",
        "    return x.reshape(x.shape[0],-1)/255.0\n",
        "\n",
        "  def incerement_grad(self,a,x):\n",
        "    for i in range(1,len(a)):\n",
        "      a[i]+=x[i]\n",
        "    return a\n",
        "\n",
        "  def initGrads(self):\n",
        "    del_w=dict()\n",
        "    del_w[1]=np.zeros((self.neurons_in_hl,self.x_train.shape[1]), dtype=np.float128)\n",
        "    for i in range(2,self.num_of_layers-1):\n",
        "      del_w[i]=np.zeros((self.neurons_in_hl,self.neurons_in_hl), dtype=np.float128)\n",
        "    del_w[self.num_of_layers-1]=np.zeros((self.y_train.shape[1],self.neurons_in_hl), dtype=np.float128)\n",
        "\n",
        "    del_b=dict()\n",
        "    for i in range(1,self.num_of_layers-1):\n",
        "      del_b[i]=np.zeros(self.neurons_in_hl, dtype=np.float128)\n",
        "    del_b[self.num_of_layers-1]=np.zeros(self.y_train.shape[1], dtype=np.float128)\n",
        "\n",
        "    return del_w,del_b\n",
        "\n",
        "\n",
        "  def forward_propagation(self,w,b,x):\n",
        "    h=dict()\n",
        "    a=dict()\n",
        "\n",
        "    h[0]=x\n",
        "\n",
        "    for k in range(1,self.num_of_layers-1):\n",
        "      a[k]=b[k]+np.dot(w[k],h[k-1])\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        h[k]=Activation.sigmoid(a[k])\n",
        "      elif(self.activation==\"tanh\"):\n",
        "        h[k]=Activation.tanh(a[k])\n",
        "      else:\n",
        "        h[k]=Activation.relu(a[k])\n",
        "\n",
        "    a[self.num_of_layers-1]=b[self.num_of_layers-1]+np.dot(w[self.num_of_layers-1],h[self.num_of_layers-2])\n",
        "    y_cap=Activation.softmax(a[self.hls+1])\n",
        "\n",
        "    return a,h,y_cap\n",
        "\n",
        "  def backward_propagation(self,h,a,y,y_cap):\n",
        "    del_a=dict()\n",
        "    del_w=dict()\n",
        "    del_b=dict()\n",
        "    del_h=dict()\n",
        "\n",
        "    del_a[self.num_of_layers-1]=-(y-y_cap)\n",
        "    for k in range(self.num_of_layers-1,0,-1):\n",
        "      del_w[k]=np.outer(del_a[k],h[k-1])\n",
        "      del_b[k]=del_a[k]\n",
        "      del_h[k-1]=np.dot(self.W[k].T,del_a[k])\n",
        "      if k>1:\n",
        "        if(self.activation==\"sigmoid\"):\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],Activation.grad_sigmoid(a[k-1]))\n",
        "        elif(self.activation==\"tanh\"):\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],Activation.grad_tanh(a[k-1]))\n",
        "        else:\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],Activation.grad_relu(a[k-1]))\n",
        "\n",
        "    return (del_w,del_b)\n",
        "\n",
        "  def stochastic_gradient_descent(self,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      del_w,del_b=self.initGrads()\n",
        "      num_of_points_seen=0\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        predictions.append(y_cap)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          self.W,self.B=UpdateParameters.update_parameters(self.W,self.B,self.eta,del_w,del_b)\n",
        "          del_w,del_b=self.initGrads()\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.W,self.y_train,np.array(predictions),self.wd)/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.valCrossEntropyLoss(self,self.W,self.B,self.x_val,self.y_val,self.wd)/x_val.shape[0])\n",
        "        valAccuracyPerEpoch.append(Accuracy.valAccuracy(self,self.W,self.B,self.x_val,self.y_val))\n",
        "      # if(math.isnan(trainLossPerEpoch[-1])):\n",
        "      #   trainLossPerEpoch[-1]=0\n",
        "      # if(math.isnan(valLossPerEpoch[-1])):\n",
        "      #   valLossPerEpoch[-1]=0\n",
        "        print(\"********************************\")\n",
        "        print(\"Epoch Number = {}\".format(iter))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        # self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "  def momentum_gradient_descent(self,beta,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    prev_uw,prev_ub=self.initGrads()\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      del_w,del_b=self.initGrads()\n",
        "      num_of_points_seen=0\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        predictions.append(y_cap)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          uw,ub=dict(),dict()\n",
        "          for i in range(1,len(del_w)):\n",
        "            uw[i]=beta*prev_uw[i]+self.eta*del_w[i]\n",
        "            ub[i]=beta*prev_ub[i]+self.eta*del_b[i]\n",
        "\n",
        "          self.W,self.B=UpdateParameters.update_parameters_mgd(self.W,self.B,uw,ub)\n",
        "          prev_uw=uw\n",
        "          prev_ub=ub\n",
        "          del_w,del_b=self.initGrads()\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.W,self.y_train,np.array(predictions),self.wd)/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.valCrossEntropyLoss(self,self.W,self.B,self.x_val,self.y_val,self.wd)/x_val.shape[0])\n",
        "        valAccuracyPerEpoch.append(Accuracy.valAccuracy(self,self.W,self.B,self.x_val,self.y_val))\n",
        "        print(\"********************************\")\n",
        "        print(\"Epoch Number = {}\".format(iter))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        # self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "  def nestrov_gradient_descent(self,beta,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    # prev_uw,prev_ub=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      prev_uw,prev_ub=self.initGrads()\n",
        "      predictions=list()\n",
        "      del_w,del_b=self.initGrads()\n",
        "      uw,ub=dict(),dict()\n",
        "      for i in range(1,len(del_w)):\n",
        "        uw[i]=beta*prev_uw[i]\n",
        "        ub[i]=beta*prev_ub[i]\n",
        "\n",
        "      w,b=UpdateParameters.update_parameters_mgd(self.W,self.B,uw,ub)\n",
        "      num_of_points_seen=0\n",
        "\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        original_selfW,original_selfB=dict(),dict()\n",
        "        A,H,y_cap=self.forward_propagation(w,b,x)\n",
        "        predictions.append(y_cap)\n",
        "        for i in range(1,len(self.W)):\n",
        "          original_selfW[i]=self.W[i]\n",
        "          self.W[i]=w[i]\n",
        "          original_selfB[i]=self.B[i]\n",
        "          self.B[i]=b[i]\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        for i in range(1,len(self.W)):\n",
        "          self.W[i]=original_selfW[i]\n",
        "          self.B[i]=original_selfB[i]\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          for i in range(1,len(del_w)):\n",
        "            uw[i]=beta*prev_uw[i]+self.eta*del_w[i]\n",
        "            ub[i]=beta*prev_ub[i]+self.eta*del_b[i]\n",
        "\n",
        "          self.W,self.B=UpdateParameters.update_parameters_mgd(self.W,self.B,uw,ub)\n",
        "          prev_uw=uw\n",
        "          prev_ub=ub\n",
        "          del_w,del_b=self.initGrads()\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.W,self.y_train,np.array(predictions),self.wd)/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.valCrossEntropyLoss(self,self.W,self.B,self.x_val,self.y_val,self.wd)/x_val.shape[0])\n",
        "        valAccuracyPerEpoch.append(Accuracy.valAccuracy(self,self.W,self.B,self.x_val,self.y_val))\n",
        "        print(\"********************************\")\n",
        "        print(\"Epoch Number = {}\".format(iter))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        # self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "\n",
        "  def rmsprop(self,beta,eps,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    vw,vb=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      del_w,del_b=self.initGrads()\n",
        "      num_of_points_seen=0\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        predictions.append(y_cap)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          for i in range(1,len(del_w)):\n",
        "            vw[i]=(beta*vw[i])+((1-beta)*(del_w[i]**2))\n",
        "            vb[i]=(beta*vb[i])+((1-beta)*(del_b[i]**2))\n",
        "\n",
        "          self.W,self.B=UpdateParameters.update_parameters_rms(self.W,self.B,self.eta,vw,vb,del_w,del_b,eps)\n",
        "          del_w,del_b=self.initGrads()\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.W,self.y_train,np.array(predictions),self.wd)/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.valCrossEntropyLoss(self,self.W,self.B,self.x_val,self.y_val,self.wd)/x_val.shape[0])\n",
        "        valAccuracyPerEpoch.append(Accuracy.valAccuracy(self,self.W,self.B,self.x_val,self.y_val))\n",
        "        print(\"********************************\")\n",
        "        print(\"Epoch Number = {}\".format(iter))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        # self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "\n",
        "  def adam(self,beta1,beta2,eps,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    vw,vb=self.initGrads()\n",
        "    mw,mb=self.initGrads()\n",
        "    mw_hat,mb_hat=self.initGrads()\n",
        "    vw_hat,vb_hat=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      del_w,del_b=self.initGrads()\n",
        "      num_of_points_seen=0\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        predictions.append(y_cap)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          for i in range(1,len(del_w)):\n",
        "            mw[i]=(beta1*mw[i])+((1-beta1)*del_w[i])\n",
        "            mb[i]=(beta1*mb[i])+((1-beta1)*del_b[i])\n",
        "            vw[i]=(beta2*vw[i])+((1-beta2)*(del_w[i]**2))\n",
        "            vb[i]=(beta2*vb[i])+((1-beta2)*(del_b[i]**2))\n",
        "\n",
        "          for i in range(1,len(del_w)):\n",
        "            mw_hat[i]=mw[i]/(1-np.power(beta1,i+1))\n",
        "            mb_hat[i]=mb[i]/(1-np.power(beta1,i+1))\n",
        "            vw_hat[i]=vw[i]/(1-np.power(beta2,i+1))\n",
        "            vb_hat[i]=vb[i]/(1-np.power(beta2,i+1))\n",
        "\n",
        "          del_w,del_b=self.initGrads()\n",
        "          self.W,self.B=UpdateParameters.update_parameters_adam(self.W,self.B,self.eta,mw_hat,mb_hat,vw_hat,vb_hat,eps)\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.W,self.y_train,np.array(predictions),self.wd)/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.valCrossEntropyLoss(self,self.W,self.B,self.x_val,self.y_val,self.wd)/x_val.shape[0])\n",
        "        valAccuracyPerEpoch.append(Accuracy.valAccuracy(self,self.W,self.B,self.x_val,self.y_val))\n",
        "        print(\"********************************\")\n",
        "        print(\"Epoch Number = {}\".format(iter))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        # wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        # self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "\n",
        "  def nadam(self,beta1,beta2,eps,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    vw,vb=self.initGrads()\n",
        "    mw,mb=self.initGrads()\n",
        "    mw_hat,mb_hat=self.initGrads()\n",
        "    vw_hat,vb_hat=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      del_w,del_b=self.initGrads()\n",
        "      num_of_points_seen=0\n",
        "      for x,y in zip(self.x_train,self.y_train):\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        predictions.append(y_cap)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        del_w=self.incerement_grad(del_w,ret[0])\n",
        "        del_b=self.incerement_grad(del_b,ret[1])\n",
        "        num_of_points_seen+=1\n",
        "\n",
        "        if(num_of_points_seen%batch_size==0):\n",
        "          for i in range(1,len(del_w)):\n",
        "            mw[i]=(beta1*mw[i])+((1-beta1)*del_w[i])\n",
        "            mb[i]=(beta1*mb[i])+((1-beta1)*del_b[i])\n",
        "            vw[i]=(beta2*vw[i])+((1-beta2)*(del_w[i]**2))\n",
        "            vb[i]=(beta2*vb[i])+((1-beta2)*(del_b[i]**2))\n",
        "\n",
        "          for i in range(1,len(del_w)):\n",
        "            mw_hat[i]=mw[i]/(1-np.power(beta1,i+1))\n",
        "            mb_hat[i]=mb[i]/(1-np.power(beta1,i+1))\n",
        "            vw_hat[i]=vw[i]/(1-np.power(beta2,i+1))\n",
        "            vb_hat[i]=vb[i]/(1-np.power(beta2,i+1))\n",
        "\n",
        "          self.W,self.B=UpdateParameters.update_parameters_nadam(self.W,self.B,self.eta,mw_hat,mb_hat,vw_hat,vb_hat,beta1,beta2,del_w,del_b,eps)\n",
        "          del_w,del_b=self.initGrads()\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.W,self.y_train,np.array(predictions),self.wd)/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.valCrossEntropyLoss(self,self.W,self.B,self.x_val,self.y_val,self.wd)/x_val.shape[0])\n",
        "        valAccuracyPerEpoch.append(Accuracy.valAccuracy(self,self.W,self.B,self.x_val,self.y_val))\n",
        "        print(\"********************************\")\n",
        "        print(\"Epoch Number = {}\".format(iter))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        # self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "\n",
        "  def modelFitting(self,beta,beta1,beta2,eps,optimizer,batch_size,loss):\n",
        "    # run_name=\"lr_{}_op_{}_bs_{}_loss_{}_wInit_{}_hls_{}_nhls_{}_ac_{}\".format(self.eta,optimizer,batch_size,loss,self.initialization,self.hls,self.neurons_in_hl, self.activation)\n",
        "    # print(\"run name = {}\".format(run_name))\n",
        "    # wandb.run.name=run_name\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    if(optimizer==\"sgd\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.stochastic_gradient_descent(batch_size)\n",
        "    elif(optimizer==\"momentum\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.momentum_gradient_descent(beta,batch_size)\n",
        "    elif(optimizer==\"nestrov\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.nestrov_gradient_descent(beta,batch_size)\n",
        "    elif(optimizer==\"rmsprop\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.rmsprop(beta,eps,batch_size)\n",
        "    elif(optimizer==\"adam\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.adam(beta1,beta2,eps,batch_size)\n",
        "    elif(optimizer==\"nadam\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.nadam(beta1,beta2,eps,batch_size)\n",
        "\n",
        "    print(trainLossPerEpoch)\n",
        "    print(trainAccuracyPerEpoch)\n",
        "    print(valLossPerEpoch)\n",
        "    print(valAccuracyPerEpoch)\n",
        "\n",
        "  # def modelFittingForBestConfig(self,beta,beta1,beta2,eps,optimizer,batch_size,loss):\n",
        "  #   w,b=dict(),dict()\n",
        "\n",
        "  #   bestConfigRun=True\n",
        "  #   if(optimizer==\"sgd\"):\n",
        "  #     w,b=self.stochastic_gradient_descent(batch_size)\n",
        "  #   elif(optimizer==\"momentum\"):\n",
        "  #     w,b=self.momentum_gradient_descent(beta,batch_size)\n",
        "  #   elif(optimizer==\"nestrov\"):\n",
        "  #     w,b=self.nestrov_gradient_descent(beta,batch_size)\n",
        "  #   elif(optimizer==\"rmsprop\"):\n",
        "  #     w,b=self.rmsprop(beta,eps,batch_size)\n",
        "  #   elif(optimizer==\"adam\"):\n",
        "  #     w,b=self.adam(beta1,beta2,eps,batch_size)\n",
        "  #   elif(optimizer==\"nadam\"):\n",
        "  #     w,b=self.nadam(beta1,beta2,eps,batch_size)\n",
        "  #   bestConfigRun=False\n",
        "\n",
        "  #   return w,b\n",
        "\n"
      ],
      "metadata": {
        "id": "clEgTY69AzJy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accuracy:\n",
        "  def trainAccuracy(y,y_pred):\n",
        "    accuracy=0\n",
        "    for i in range(y.shape[0]):\n",
        "      if(np.argmax(y[i])==np.argmax(y_pred[i])):\n",
        "        accuracy+=1\n",
        "\n",
        "    return accuracy/y.shape[0]\n",
        "\n",
        "  def valAccuracy(Model,w,b,x,y):\n",
        "    accuracy=0\n",
        "    for i in range(x.shape[0]):\n",
        "      a,h,y_pred=Model.forward_propagation(w,b,x[i])\n",
        "      if(np.argmax(y[i])==np.argmax(y_pred)):\n",
        "        accuracy+=1\n",
        "\n",
        "    return accuracy/x.shape[0]\n",
        "\n",
        "  def testAccuracy(y,y_pred):\n",
        "    accuracy=0\n",
        "    for i in range(y.shape[0]):\n",
        "      if(np.argmax(y[i])==np.argmax(y_pred[i])):\n",
        "        accuracy+=1\n",
        "\n",
        "    return accuracy/y.shape[0]\n"
      ],
      "metadata": {
        "id": "1SCDxTm449u1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  def trainCrossEntropyLoss(w,y,yPred,wd):\n",
        "    val=-np.sum(y*np.log(yPred+1e-9))\n",
        "    sum=0\n",
        "    for i in range(1,len(w)):\n",
        "      sum+=np.sum(np.square(w[i]))\n",
        "    return val+wd*sum\n",
        "\n",
        "  def valCrossEntropyLoss(Model,w,b,x,y,wd):\n",
        "    yPred1=list()\n",
        "    for i in range(x.shape[0]):\n",
        "      a,h,y_pred=Model.forward_propagation(w,b,x[i])\n",
        "      yPred1.append(y_pred)\n",
        "    yPred=np.array(yPred1)\n",
        "    val=-np.sum(y*np.log(yPred+1e-9))\n",
        "    sum=0\n",
        "    for i in range(1,len(w)):\n",
        "      sum+=np.sum(np.square(w[i]))\n",
        "    return val+wd*sum\n"
      ],
      "metadata": {
        "id": "AdETbhkL8WGe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  wandb.init(project=\"Pritam CS6910 - Assignment 1\")\n",
        "  config=wandb.config\n",
        "\n",
        "  Model=FeedForwardNeuralNetwork(x_train,y_train,x_val,y_val,hls=config.number_of_hidden_layers,neurons_in_hl=config.num_neurons_in_hidden_layers,activation=config.activation,initialization=config.initialization,epochs=config.epochs,eta=config.learning_rate,wd=config.weight_decay)\n",
        "  Model.modelFitting(beta=config.beta,beta1=0.9,beta2=0.999,eps=1e-5,optimizer=config.optimizer,batch_size=config.batch_size,loss=config.loss)\n",
        "\n",
        "# Model=FeedForwardNeuralNetwork(x_train,y_train,x_val,y_val,hls=4,neurons_in_hl=32,activation=\"xavier\",initialization=\"tanh\",epochs=5,eta=1e-2,wd=0.0005)\n",
        "# Model.modelFitting(beta=0.9,beta1=0.9,beta2=0.999,eps=1e-5,optimizer=\"adam\",batch_size=32,loss=\"cross_entropy\")\n",
        "\n",
        "\n",
        "sweep_configuration = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'ACCURACY VS EPOCH',\n",
        "    'metric': {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'validation_accuracy'\n",
        "        },\n",
        "    'parameters': {\n",
        "        'initialization': {'values': ['xavier','random']},\n",
        "        'number_of_hidden_layers' : {'values' : [3,4,5]},\n",
        "        'num_neurons_in_hidden_layers' : {'values' : [32,64,128]},\n",
        "\n",
        "        'learning_rate': {'values':[1e-1,5e-1,1e-2,1e-3,5e-3,1e-4]},\n",
        "        'beta' : {'values' : [0.9,0.999]},\n",
        "        'optimizer' : {'values' : ['sgd','momentum','rmsprop','adam','nadam','nestrov']},\n",
        "\n",
        "        'batch_size': {'values': [16,32,64,128]},\n",
        "        'epochs': {'values': [5,10]},\n",
        "        'loss' : {'values' : ['cross_entropy']},\n",
        "        'activation' : {'values' : ['sigmoid','relu','tanh']},\n",
        "        'weight_decay' : {'values' : [0, 0.0005,0.5]}\n",
        "       }\n",
        "    }\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_configuration,project='Pritam CS6910 - Assignment 1')\n",
        "\n",
        "wandb.agent(sweep_id , function = main , count = 100)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0lcoFIkA2Fl",
        "outputId": "5b7ad797-5264-4994-983a-cbd884bd7e1b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************************\n",
            "Epoch Number = 0\n",
            "Training Accuracy = 0.7943518518518519\n",
            "Validation Accuracy = 0.8335\n",
            "********************************\n",
            "Epoch Number = 1\n",
            "Training Accuracy = 0.8459444444444445\n",
            "Validation Accuracy = 0.849\n",
            "********************************\n",
            "Epoch Number = 2\n",
            "Training Accuracy = 0.8572592592592593\n",
            "Validation Accuracy = 0.8495\n",
            "********************************\n",
            "Epoch Number = 3\n",
            "Training Accuracy = 0.8630185185185185\n",
            "Validation Accuracy = 0.853\n",
            "********************************\n",
            "Epoch Number = 4\n",
            "Training Accuracy = 0.8655925925925926\n",
            "Validation Accuracy = 0.8535\n",
            "[0.57266471736044339625, 0.4288044132957514808, 0.39603850423110504352, 0.37722382155190718137, 0.3650370192108048766]\n",
            "[0.7943518518518519, 0.8459444444444445, 0.8572592592592593, 0.8630185185185185, 0.8655925925925926]\n",
            "[0.4721641062994379819, 0.42708564440971012705, 0.42082993975010355304, 0.4121855329937187667, 0.40627843548225165015]\n",
            "[0.8335, 0.849, 0.8495, 0.853, 0.8535]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Model=FeedForwardNeuralNetwork(x_train,y_train,x_val,y_val,hls=4,neurons_in_hl=64,activation=\"tanh\",initialization=\"xavier\",epochs=10,eta=0.001)\n",
        "w,b=Model.modelFittingForBestConfig(beta=0.9.beta,beta1=0.9,beta2=0.999,eps=1e-5,optimizer=\"nestrov\",batch_size=32,loss=\"cross_entropy\")\n",
        "\n",
        "y_test1=Model.one_hot_encoding(y_test)\n",
        "x_test1=Model.input_flattening(x_test)\n",
        "\n",
        "yPred=list()\n",
        "for i in range(len(y_test1)):\n",
        "  _,_,y_cap=Model.forward_propagation(w,b,x_test1[i])\n",
        "  yPred.append(np.array(y_cap))\n",
        "\n",
        "test_accuracy=Accuracy.testAccuracy(y_test1,yPred)\n",
        "print(\"test accuracy for the best model = {}\".format(test_accuracy))\n",
        "\n",
        "output_class=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n"
      ],
      "metadata": {
        "id": "z5POOqbESI01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}