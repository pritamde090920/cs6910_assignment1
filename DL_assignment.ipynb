{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng9maxlhKYBC",
        "outputId": "2684fc88-fd80-49c0-a97a-74c3041497fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login f1b24d39429069d8dc1aa969c1c80ada9a3f743c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwDtem2bKi6_",
        "outputId": "85471a7f-fd15-45a2-c2c2-31dba8b61a69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1"
      ],
      "metadata": {
        "id": "G_t3iXTpKulA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from keras.datasets import fashion_mnist\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\n",
        "output_class=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "wandb.init(project=\"Pritam CS6910 - Assignment 1\",name=\"Question 1\")\n",
        "\n",
        "img,getPlot=plt.subplots(2,5,figsize=(20,6))\n",
        "getPlot=getPlot.flatten()\n",
        "output_images=[]\n",
        "for i in range(10):\n",
        "  imgClass=np.argmax(y_train==i)\n",
        "  getPlot[i].imshow(x_train[imgClass],cmap=\"gray\")\n",
        "  getPlot[i].set_title(output_class[i])\n",
        "  img=wandb.Image(x_train[imgClass],caption=[output_class[i]])\n",
        "  output_images.append(img)\n",
        "wandb.log({\"Question 1\":output_images})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "RKIyJtWkKq9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2"
      ],
      "metadata": {
        "id": "ueLlctlCKw-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from keras.datasets import fashion_mnist\n",
        "from tqdm import tqdm\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\n",
        "validation_ratio=0.1\n",
        "num_validation_samples=int(validation_ratio*x_train.shape[0])\n",
        "\n",
        "validation_indices=np.random.choice(x_train.shape[0],num_validation_samples,replace=False)\n",
        "\n",
        "x_val,y_val=x_train[validation_indices],y_train[validation_indices]\n",
        "x_train, y_train=np.delete(x_train,validation_indices,axis=0),np.delete(y_train,validation_indices,axis=0)"
      ],
      "metadata": {
        "id": "Zy9xbtFXKz56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation:\n",
        "  def sigmoid(x):\n",
        "    clipped_x=np.clip(x,-150,150)\n",
        "    return 1/(1+np.exp(-clipped_x))\n",
        "\n",
        "  def grad_sigmoid(x):\n",
        "    clipped_x=np.clip(x,-150,150)\n",
        "    s=1/(1+np.exp(-clipped_x))\n",
        "    return s*(1-s)\n",
        "\n",
        "  def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "  def grad_tanh(x):\n",
        "    return 1-(np.tanh(x)**2)\n",
        "\n",
        "  def relu(x):\n",
        "    return np.maximum(x,0)\n",
        "\n",
        "  def grad_relu(x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "  def softmax(a):\n",
        "    # max_a=np.max(a)\n",
        "    exp_a=np.exp(a-np.max(a))\n",
        "    sum_exp=np.sum(exp_a)\n",
        "    return exp_a/sum_exp\n",
        "    # max_per_row = np.max(a, axis=1, keepdims=True)\n",
        "    # a -= max_per_row\n",
        "    # exp_data = np.exp(a)\n",
        "    # softmax_data = exp_data / np.sum(exp_data, axis=1, keepdims=True)\n",
        "    # print(data.shape)\n",
        "    # max_per_row = np.max(data, axis=1, keepdims=True)\n",
        "    # data -= max_per_row\n",
        "    # data=np.clip(data,-150,150)\n",
        "    # exp_data = np.exp(data)\n",
        "    # softmax_data = exp_data / np.sum(exp_data)\n",
        "    # return softmax_data\n",
        "    # exp_a=np.exp(a-np.max(a,axis=1))\n",
        "    # return exp_a/np.sum(exp_a,axis=1)\n",
        "    # clipped_a=np.clip(a,-500,500)\n",
        "    # return np.exp(clipped_a)/np.sum(np.exp(clipped_a),axis=0)"
      ],
      "metadata": {
        "id": "7bV5ZAZ_K198"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpdateParameters:\n",
        "  def update_parameters(W,B,eta,del_w,del_b,wd):\n",
        "    for i in range(1,len(del_w)+1):\n",
        "      W[i]-=(eta*del_w[i]+eta*wd*W[i])\n",
        "    for i in range(1,len(del_b)+1):\n",
        "      B[i]-=eta*del_b[i]\n",
        "    return W,B\n",
        "\n",
        "  def update_parameters_mgd(W,B,del_w,del_b,eta,wd):\n",
        "    for i in range(1,len(del_w)):\n",
        "      W[i]-=(del_w[i]+eta*wd*W[i])\n",
        "    for i in range(1,len(del_b)):\n",
        "      B[i]-=del_b[i]\n",
        "    return W,B\n",
        "\n",
        "  def update_parameters_rms(W,B,eta,vw,vb,del_w,del_b,eps,wd):\n",
        "    for i in range(1,len(vw)):\n",
        "      updated_eta=eta/(np.sqrt(np.sum(vw[i]))+eps)\n",
        "      W[i]-=(updated_eta*del_w[i]+eta*wd*W[i])\n",
        "    for i in range(1,len(vb)):\n",
        "      updated_eta=eta/(np.sqrt(np.sum(vb[i]))+eps)\n",
        "      B[i]-=updated_eta*del_b[i]\n",
        "    return W,B\n",
        "\n",
        "  def update_parameters_adam(W,B,eta,mw_hat,mb_hat,vw_hat,vb_hat,eps,wd):\n",
        "    for i in range(1,len(vw_hat)):\n",
        "      # updated_eta=eta/(np.sqrt(vw_hat[i]+eps))\n",
        "      W[i]-=(eta*mw_hat[i]/(np.sqrt(vw_hat[i]+eps))+eta*wd*W[i])\n",
        "    for i in range(1,len(vb_hat)):\n",
        "      # updated_eta=eta/(np.sqrt(vb_hat[i]+eps))\n",
        "      B[i]-=eta*mb_hat[i]/(np.sqrt(vb_hat[i]+eps))\n",
        "    return W,B\n",
        "\n",
        "  def update_parameters_nadam(W,B,eta,mw_hat,mb_hat,vw_hat,vb_hat,beta1,beta2,del_w,del_b,eps,wd):\n",
        "    for i in range(1,len(vw_hat)):\n",
        "      W[i]-=((eta/(np.sqrt(vw_hat[i]+eps)))*((beta1*mw_hat[i])+(((1-beta1)*del_w[i])/(1-beta1**(i+1))))+eta*wd*W[i])\n",
        "    for i in range(1,len(vb_hat)):\n",
        "      B[i]-=(eta/(np.sqrt(vb_hat[i]+eps)))*((beta1*mb_hat[i])+(((1-beta1)*del_b[i])/(1-beta1**(i+1))))\n",
        "    return W,B\n",
        "\n",
        "  def normalizeParameters(W,B):\n",
        "    for i in range(1,len(W)):\n",
        "      val=np.linalg.norm(W[i])\n",
        "      if(val>1.0):\n",
        "        W[i]=W[i]/val\n",
        "    for i in range(1,len(B)):\n",
        "      val=np.linalg.norm(B[i])\n",
        "      if(val>1.0):\n",
        "        B[i]=B[i]/val\n",
        "    return W,B\n"
      ],
      "metadata": {
        "id": "v-F4qikGK4vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParamaterInitialization:\n",
        "  def initializeW(initialization,num_of_layers,neurons_in_hl,x_train_shape,y_train_shape):\n",
        "    w=dict()\n",
        "    if(initialization==\"random\"):\n",
        "      w[1]=np.random.randn(neurons_in_hl,x_train_shape).astype(np.float128)\n",
        "      for i in range(2,num_of_layers-1):\n",
        "        w[i]=np.random.randn(neurons_in_hl,neurons_in_hl).astype(np.float128)\n",
        "      w[num_of_layers-1]=np.random.randn(y_train_shape,neurons_in_hl).astype(np.float128)\n",
        "\n",
        "    else:\n",
        "      w[1]=(np.random.randn(neurons_in_hl,x_train_shape)/np.sqrt(x_train_shape)).astype(np.float128)\n",
        "      for i in range(2,num_of_layers-1):\n",
        "        w[i]=(np.random.randn(neurons_in_hl, neurons_in_hl)/np.sqrt(neurons_in_hl)).astype(np.float128)\n",
        "      w[num_of_layers-1]=(np.random.randn(y_train_shape,neurons_in_hl)/np.sqrt(neurons_in_hl)).astype(np.float128)\n",
        "\n",
        "    return w\n",
        "\n",
        "  def initializeB(initialization,num_of_layers,neurons_in_hl,x_train_shape,y_train_shape):\n",
        "    b=dict()\n",
        "    if(initialization==\"random\"):\n",
        "      for i in range(1,num_of_layers-1):\n",
        "        b[i]=np.random.randn(neurons_in_hl).astype(np.float128)\n",
        "      b[num_of_layers-1]=np.random.randn(y_train_shape).astype(np.float128)\n",
        "\n",
        "    else:\n",
        "      for i in range(1,num_of_layers-1):\n",
        "        b[i]=(np.random.randn(neurons_in_hl)/np.sqrt(neurons_in_hl)).astype(np.float128)\n",
        "      b[num_of_layers-1]=(np.random.randn(y_train_shape)/np.sqrt(neurons_in_hl)).astype(np.float128)\n",
        "\n",
        "    return b"
      ],
      "metadata": {
        "id": "B4mKtPhuK6oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNeuralNetwork:\n",
        "\n",
        "  epochs=0\n",
        "  hls=0\n",
        "  neurons_in_hl=0\n",
        "  eta=0.0\n",
        "  activation=0\n",
        "  num_of_layers=0\n",
        "  x_train=np.zeros(784)\n",
        "  y_train=np.zeros(10)\n",
        "  W=dict()\n",
        "  B=dict()\n",
        "  wd=0.0\n",
        "  bestConfigRun=False\n",
        "\n",
        "  def __init__ (self,x_train1,y_train1,x_val1,y_val1,hls,neurons_in_hl,activation,initialization,epochs,eta,wd):\n",
        "    self.epochs=epochs\n",
        "    self.hls=hls\n",
        "    self.neurons_in_hl=neurons_in_hl\n",
        "    self.eta=eta\n",
        "    self.activation=activation\n",
        "    self.num_of_layers=hls+2\n",
        "    self.initialization=initialization\n",
        "    self.wd=wd\n",
        "\n",
        "    self.x_train=self.input_flattening(x_train1)\n",
        "    self.y_train=self.one_hot_encoding(y_train1)\n",
        "\n",
        "    self.x_val=self.input_flattening(x_val1)\n",
        "    self.y_val=self.one_hot_encoding(y_val1)\n",
        "\n",
        "    self.W=ParamaterInitialization.initializeW(self.initialization,self.num_of_layers,self.neurons_in_hl,self.x_train.shape[1],self.y_train.shape[1])\n",
        "    self.B=ParamaterInitialization.initializeB(self.initialization,self.num_of_layers,self.neurons_in_hl,self.x_train.shape[1],self.y_train.shape[1])\n",
        "\n",
        "\n",
        "  def one_hot_encoding(self,y):\n",
        "    temp=list()\n",
        "    for i in range(y.shape[0]):\n",
        "      vector=np.zeros(10)\n",
        "      vector[y[i]]=1\n",
        "      temp.append(vector)\n",
        "    return np.array(temp)\n",
        "\n",
        "  def input_flattening(self,x):\n",
        "    return x.reshape(x.shape[0],-1)/255.0\n",
        "\n",
        "  def incerement_grad(self,a,x):\n",
        "    for i in range(1,len(a)+1):\n",
        "      a[i]+=x[i]\n",
        "    return a\n",
        "\n",
        "  def initGrads(self):\n",
        "    del_w=dict()\n",
        "    del_w[1]=np.zeros((self.neurons_in_hl,self.x_train.shape[1]), dtype=np.float128)\n",
        "    for i in range(2,self.num_of_layers-1):\n",
        "      del_w[i]=np.zeros((self.neurons_in_hl,self.neurons_in_hl), dtype=np.float128)\n",
        "    del_w[self.num_of_layers-1]=np.zeros((self.y_train.shape[1],self.neurons_in_hl), dtype=np.float128)\n",
        "\n",
        "    del_b=dict()\n",
        "    for i in range(1,self.num_of_layers-1):\n",
        "      del_b[i]=np.zeros(self.neurons_in_hl, dtype=np.float128)\n",
        "    del_b[self.num_of_layers-1]=np.zeros(self.y_train.shape[1], dtype=np.float128)\n",
        "\n",
        "    return del_w,del_b\n",
        "\n",
        "\n",
        "  def forward_propagation_test(self,w,b,x):\n",
        "    h=dict()\n",
        "    a=dict()\n",
        "    h[0]=x\n",
        "\n",
        "    for k in range(1,self.num_of_layers-1):\n",
        "      a[k]=b[k]+np.dot(w[k],h[k-1])\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        h[k]=Activation.sigmoid(a[k])\n",
        "      elif(self.activation==\"tanh\"):\n",
        "        h[k]=Activation.tanh(a[k])\n",
        "      else:\n",
        "        h[k]=Activation.relu(a[k])\n",
        "\n",
        "    a[self.num_of_layers-1]=b[self.num_of_layers-1]+np.dot(w[self.num_of_layers-1],h[self.num_of_layers-2])\n",
        "    y_cap=Activation.softmax(a[self.hls+1])\n",
        "\n",
        "    return a,h,y_cap\n",
        "\n",
        "  def forward_propagation(self,w,b,x):\n",
        "    h=dict()\n",
        "    a=dict()\n",
        "    b1=dict()\n",
        "\n",
        "    h[0]=x\n",
        "\n",
        "    for k in range(1,self.num_of_layers-1):\n",
        "      b1[k]=b[k].reshape(1,-1)\n",
        "      b1[k]=np.repeat(b1[k],x.shape[1],axis=0).transpose()\n",
        "      a[k]=b1[k]+np.matmul(w[k],h[k-1])\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        h[k]=Activation.sigmoid(a[k])\n",
        "      elif(self.activation==\"tanh\"):\n",
        "        h[k]=Activation.tanh(a[k])\n",
        "      else:\n",
        "        h[k]=Activation.relu(a[k])\n",
        "\n",
        "    b1[self.num_of_layers-1]=b[self.num_of_layers-1].reshape(1,-1)\n",
        "    b1[self.num_of_layers-1]=np.repeat(b1[self.num_of_layers-1],x.shape[1],axis=0).transpose()\n",
        "    a[self.num_of_layers-1]=b1[self.num_of_layers-1]+np.matmul(w[self.num_of_layers-1],h[self.num_of_layers-2])\n",
        "    temp=a[self.num_of_layers-1].transpose()\n",
        "    l=list()\n",
        "    for i in range(len(temp)):\n",
        "      l.append(Activation.softmax(temp[i]))\n",
        "    y_cap=np.array(l)\n",
        "\n",
        "    return a,h,y_cap.T\n",
        "\n",
        "  def backward_propagation(self,h,a,y,y_cap):\n",
        "    del_a=dict()\n",
        "    del_w=dict()\n",
        "    del_b=dict()\n",
        "    del_h=dict()\n",
        "\n",
        "    del_a[self.num_of_layers-1]=-(y-y_cap)\n",
        "    for k in range(self.num_of_layers-1,0,-1):\n",
        "      del_w[k]=np.matmul(del_a[k],h[k-1].T)\n",
        "      del_b[k]=np.sum(del_a[k],axis=1)\n",
        "      del_h[k-1]=np.matmul(self.W[k].T,del_a[k])\n",
        "      if k>1:\n",
        "        if(self.activation==\"sigmoid\"):\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],Activation.grad_sigmoid(a[k-1]))\n",
        "        elif(self.activation==\"tanh\"):\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],Activation.grad_tanh(a[k-1]))\n",
        "        else:\n",
        "          del_a[k-1]=np.multiply(del_h[k-1],Activation.grad_relu(a[k-1]))\n",
        "\n",
        "    return (del_w,del_b)\n",
        "\n",
        "  def stochastic_gradient_descent(self,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      for i in range(0,self.x_train.shape[0],batch_size):\n",
        "        x=self.x_train[i:i+batch_size].T\n",
        "        y=self.y_train[i:i+batch_size].T\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        self.W,self.B=UpdateParameters.update_parameters(self.W,self.B,self.eta,ret[0],ret[1],self.wd)\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        predictions=list()\n",
        "        for i in range(self.x_train.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_train[i])\n",
        "          predictions.append(val)\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_train,np.array(predictions))/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        predictions=list()\n",
        "        for i in range(self.x_val.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_val[i])\n",
        "          predictions.append(val)\n",
        "        valAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_val,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_val,np.array(predictions))/x_val.shape[0])\n",
        "      # if(math.isnan(trainLossPerEpoch[-1])):\n",
        "      #   trainLossPerEpoch[-1]=0\n",
        "      # if(math.isnan(valLossPerEpoch[-1])):\n",
        "      # #   valLossPerEpoch[-1]=0\n",
        "        print(\"\\nEpoch Number = {}\".format(iter+1))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        print(\"Training Loss = {}\".format(trainLossPerEpoch[-1]))\n",
        "        print(\"Validation Loss = {}\".format(valLossPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "  def momentum_gradient_descent(self,beta,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    prev_uw,prev_ub=self.initGrads()\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      for i in range(0,self.x_train.shape[0],batch_size):\n",
        "        x=self.x_train[i:i+batch_size].T\n",
        "        y=self.y_train[i:i+batch_size].T\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        uw,ub=dict(),dict()\n",
        "        for i in range(1,len(ret[0])):\n",
        "          uw[i]=beta*prev_uw[i]+self.eta*ret[0][i]\n",
        "          ub[i]=beta*prev_ub[i]+self.eta*ret[1][i]\n",
        "\n",
        "        self.W,self.B=UpdateParameters.update_parameters_mgd(self.W,self.B,uw,ub,self.eta,self.wd)\n",
        "        prev_uw=uw\n",
        "        prev_ub=ub\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        predictions=list()\n",
        "        for i in range(self.x_train.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_train[i])\n",
        "          predictions.append(val)\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_train,np.array(predictions))/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        predictions=list()\n",
        "        for i in range(self.x_val.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_val[i])\n",
        "          predictions.append(val)\n",
        "        valAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_val,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_val,np.array(predictions))/x_val.shape[0])\n",
        "        print(\"\\nEpoch Number = {}\".format(iter+1))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        print(\"Training Loss = {}\".format(trainLossPerEpoch[-1]))\n",
        "        print(\"Validation Loss = {}\".format(valLossPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "  def nestrov_gradient_descent(self,beta,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    prev_uw,prev_ub=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      uw,ub=dict(),dict()\n",
        "      for i in range(1,len(prev_uw)):\n",
        "        uw[i]=beta*prev_uw[i]\n",
        "        ub[i]=beta*prev_ub[i]\n",
        "\n",
        "      w,b=UpdateParameters.update_parameters_mgd(self.W,self.B,uw,ub,self.eta,self.wd)\n",
        "\n",
        "      for i in range(0,self.x_train.shape[0],batch_size):\n",
        "        original_selfW,original_selfB=dict(),dict()\n",
        "        x=self.x_train[i:i+batch_size].T\n",
        "        y=self.y_train[i:i+batch_size].T\n",
        "        A,H,y_cap=self.forward_propagation(w,b,x)\n",
        "        for i in range(1,len(self.W)):\n",
        "          original_selfW[i]=self.W[i]\n",
        "          self.W[i]=w[i]\n",
        "          original_selfB[i]=self.B[i]\n",
        "          self.B[i]=b[i]\n",
        "        ret=self.backward_propagation(H,A,y,y_cap)\n",
        "        for i in range(1,len(self.W)):\n",
        "          self.W[i]=original_selfW[i]\n",
        "          self.B[i]=original_selfB[i]\n",
        "\n",
        "        for i in range(1,len(ret[0])):\n",
        "          uw[i]=beta*prev_uw[i]+self.eta*ret[0][i]\n",
        "          ub[i]=beta*prev_ub[i]+self.eta*ret[1][i]\n",
        "\n",
        "        self.W,self.B=UpdateParameters.update_parameters_mgd(self.W,self.B,uw,ub,self.eta,self.wd)\n",
        "        prev_uw=uw\n",
        "        prev_ub=ub\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        predictions=list()\n",
        "        for i in range(self.x_train.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_train[i])\n",
        "          predictions.append(val)\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_train,np.array(predictions))/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        predictions=list()\n",
        "        for i in range(self.x_val.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_val[i])\n",
        "          predictions.append(val)\n",
        "        valAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_val,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_val,np.array(predictions))/x_val.shape[0])\n",
        "        print(\"\\nEpoch Number = {}\".format(iter+1))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        print(\"Training Loss = {}\".format(trainLossPerEpoch[-1]))\n",
        "        print(\"Validation Loss = {}\".format(valLossPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "\n",
        "  def rmsprop(self,beta,eps,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    vw,vb=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      for i in range(0,self.x_train.shape[0],batch_size):\n",
        "        x=self.x_train[i:i+batch_size].T\n",
        "        y=self.y_train[i:i+batch_size].T\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        _ret=self.backward_propagation(H,A,y,y_cap)\n",
        "\n",
        "        ret=[None,None]\n",
        "        ret[0],ret[1]=UpdateParameters.normalizeParameters(_ret[0],_ret[1])\n",
        "        for i in range(1,len(ret[0])):\n",
        "          vw[i]=(beta*vw[i])+((1-beta)*(np.square(ret[0][i])))\n",
        "          vb[i]=(beta*vb[i])+((1-beta)*(np.square(ret[1][i])))\n",
        "\n",
        "        self.W,self.B=UpdateParameters.update_parameters_rms(self.W,self.B,self.eta,vw,vb,ret[0],ret[1],eps,self.wd)\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        predictions=list()\n",
        "        for i in range(self.x_train.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_train[i])\n",
        "          predictions.append(val)\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_train,np.array(predictions))/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        predictions=list()\n",
        "        for i in range(self.x_val.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_val[i])\n",
        "          predictions.append(val)\n",
        "        valAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_val,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_val,np.array(predictions))/x_val.shape[0])\n",
        "        print(\"\\nEpoch Number = {}\".format(iter+1))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        print(\"Training Loss = {}\".format(trainLossPerEpoch[-1]))\n",
        "        print(\"Validation Loss = {}\".format(valLossPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "\n",
        "  def adam(self,beta1,beta2,eps,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    vw,vb=self.initGrads()\n",
        "    mw,mb=self.initGrads()\n",
        "    mw_hat,mb_hat=self.initGrads()\n",
        "    vw_hat,vb_hat=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      for i in range(0,self.x_train.shape[0],batch_size):\n",
        "        x=self.x_train[i:i+batch_size].T\n",
        "        y=self.y_train[i:i+batch_size].T\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        _ret=self.backward_propagation(H,A,y,y_cap)\n",
        "\n",
        "        ret=[None,None]\n",
        "        ret[0],ret[1]=UpdateParameters.normalizeParameters(_ret[0],_ret[1])\n",
        "        for i in range(1,len(ret[0])):\n",
        "          mw[i]=(beta1*mw[i])+((1-beta1)*ret[0][i])\n",
        "          mb[i]=(beta1*mb[i])+((1-beta1)*ret[1][i])\n",
        "          vw[i]=(beta2*vw[i])+((1-beta2)*(np.square(ret[0][i])))\n",
        "          vb[i]=(beta2*vb[i])+((1-beta2)*(np.square(ret[1][i])))\n",
        "\n",
        "        for i in range(1,len(ret[0])):\n",
        "          mw_hat[i]=mw[i]/(1-np.power(beta1,i+1))\n",
        "          mb_hat[i]=mb[i]/(1-np.power(beta1,i+1))\n",
        "          vw_hat[i]=vw[i]/(1-np.power(beta2,i+1))\n",
        "          vb_hat[i]=vb[i]/(1-np.power(beta2,i+1))\n",
        "\n",
        "        self.W,self.B=UpdateParameters.update_parameters_adam(self.W,self.B,self.eta,mw_hat,mb_hat,vw_hat,vb_hat,eps,self.wd)\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        predictions=list()\n",
        "        for i in range(self.x_train.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_train[i])\n",
        "          predictions.append(val)\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_train,np.array(predictions))/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        predictions=list()\n",
        "        for i in range(self.x_val.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_val[i])\n",
        "          predictions.append(val)\n",
        "        valAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_val,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_val,np.array(predictions))/x_val.shape[0])\n",
        "        print(\"\\nEpoch Number = {}\".format(iter+1))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        print(\"Training Loss = {}\".format(trainLossPerEpoch[-1]))\n",
        "        print(\"Validation Loss = {}\".format(valLossPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "\n",
        "  def nadam(self,beta1,beta2,eps,batch_size):\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    vw,vb=self.initGrads()\n",
        "    mw,mb=self.initGrads()\n",
        "    mw_hat,mb_hat=self.initGrads()\n",
        "    vw_hat,vb_hat=self.initGrads()\n",
        "\n",
        "    for iter in range(self.epochs):\n",
        "      predictions=list()\n",
        "      for i in range(0,self.x_train.shape[0],batch_size):\n",
        "        x=self.x_train[i:i+batch_size].T\n",
        "        y=self.y_train[i:i+batch_size].T\n",
        "        A,H,y_cap=self.forward_propagation(self.W,self.B,x)\n",
        "        _ret=self.backward_propagation(H,A,y,y_cap)\n",
        "\n",
        "        ret=[None,None]\n",
        "        ret[0],ret[1]=UpdateParameters.normalizeParameters(_ret[0],_ret[1])\n",
        "        for i in range(1,len(ret[0])):\n",
        "          mw[i]=(beta1*mw[i])+((1-beta1)*ret[0][i])\n",
        "          mb[i]=(beta1*mb[i])+((1-beta1)*ret[1][i])\n",
        "          vw[i]=(beta2*vw[i])+((1-beta2)*(np.square(ret[0][i])))\n",
        "          vb[i]=(beta2*vb[i])+((1-beta2)*(np.square(ret[1][i])))\n",
        "\n",
        "        for i in range(1,len(ret[0])):\n",
        "          mw_hat[i]=mw[i]/(1-np.power(beta1,i+1))\n",
        "          mb_hat[i]=mb[i]/(1-np.power(beta1,i+1))\n",
        "          vw_hat[i]=vw[i]/(1-np.power(beta2,i+1))\n",
        "          vb_hat[i]=vb[i]/(1-np.power(beta2,i+1))\n",
        "\n",
        "        self.W,self.B=UpdateParameters.update_parameters_nadam(self.W,self.B,self.eta,mw_hat,mb_hat,vw_hat,vb_hat,beta1,beta2,ret[0],ret[1],eps,self.wd)\n",
        "\n",
        "      if(self.bestConfigRun==False):\n",
        "        predictions=list()\n",
        "        for i in range(self.x_train.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_train[i])\n",
        "          predictions.append(val)\n",
        "        trainLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_train,np.array(predictions))/x_train.shape[0])\n",
        "        trainAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_train,np.array(predictions)))\n",
        "        predictions=list()\n",
        "        for i in range(self.x_val.shape[0]):\n",
        "          _,_,val=self.forward_propagation_test(self.W,self.B,self.x_val[i])\n",
        "          predictions.append(val)\n",
        "        valAccuracyPerEpoch.append(Accuracy.trainAccuracy(self.y_val,np.array(predictions)))\n",
        "        valLossPerEpoch.append(Loss.trainCrossEntropyLoss(self.y_val,np.array(predictions))/x_val.shape[0])\n",
        "        print(\"\\nEpoch Number = {}\".format(iter+1))\n",
        "        print(\"Training Accuracy = {}\".format(trainAccuracyPerEpoch[-1]))\n",
        "        print(\"Validation Accuracy = {}\".format(valAccuracyPerEpoch[-1]))\n",
        "        print(\"Training Loss = {}\".format(trainLossPerEpoch[-1]))\n",
        "        print(\"Validation Loss = {}\".format(valLossPerEpoch[-1]))\n",
        "        wandb.log({\"training_accuracy\":trainAccuracyPerEpoch[-1],\"validation_accuracy\":valAccuracyPerEpoch[-1],\"training_loss\":trainLossPerEpoch[-1],\"validation_loss\":valLossPerEpoch[-1],\"Epoch\":iter})\n",
        "        self.W,self.B=UpdateParameters.normalizeParameters(self.W,self.B)\n",
        "\n",
        "    if(self.bestConfigRun==False):\n",
        "      return trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch\n",
        "    else:\n",
        "      return self.W,self.B\n",
        "\n",
        "\n",
        "  def modelFitting(self,beta,beta1,beta2,eps,optimizer,batch_size,loss):\n",
        "    run=\"LR_{}_OP_{}_EP_{}_BS_{}_INIT_{}_HL_{}_NHL_{}_AC_{}_WD_{}\".format(self.eta,optimizer,self.epochs,batch_size,self.initialization,self.hls,self.neurons_in_hl,self.activation,self.wd)\n",
        "    print(\"run name = {}\".format(run))\n",
        "    wandb.run.name=run\n",
        "    trainLossPerEpoch=list()\n",
        "    trainAccuracyPerEpoch=list()\n",
        "    valLossPerEpoch=list()\n",
        "    valAccuracyPerEpoch=list()\n",
        "\n",
        "    if(optimizer==\"sgd\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.stochastic_gradient_descent(batch_size)\n",
        "    elif(optimizer==\"momentum\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.momentum_gradient_descent(beta,batch_size)\n",
        "    elif(optimizer==\"nestrov\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.nestrov_gradient_descent(beta,batch_size)\n",
        "    elif(optimizer==\"rmsprop\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.rmsprop(beta,eps,batch_size)\n",
        "    elif(optimizer==\"adam\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.adam(beta1,beta2,eps,batch_size)\n",
        "    elif(optimizer==\"nadam\"):\n",
        "      trainLossPerEpoch,trainAccuracyPerEpoch,valLossPerEpoch,valAccuracyPerEpoch=self.nadam(beta1,beta2,eps,batch_size)\n",
        "\n",
        "    # print(trainLossPerEpoch)\n",
        "    # print(trainAccuracyPerEpoch)\n",
        "    # print(valLossPerEpoch)\n",
        "    # print(valAccuracyPerEpoch)\n",
        "\n",
        "  # def modelFittingForBestConfig(self,beta,beta1,beta2,eps,optimizer,batch_size,loss):\n",
        "  #   w,b=dict(),dict()\n",
        "\n",
        "  #   bestConfigRun=True\n",
        "  #   if(optimizer==\"sgd\"):\n",
        "  #     w,b=self.stochastic_gradient_descent(batch_size)\n",
        "  #   elif(optimizer==\"momentum\"):\n",
        "  #     w,b=self.momentum_gradient_descent(beta,batch_size)\n",
        "  #   elif(optimizer==\"nestrov\"):\n",
        "  #     w,b=self.nestrov_gradient_descent(beta,batch_size)\n",
        "  #   elif(optimizer==\"rmsprop\"):\n",
        "  #     w,b=self.rmsprop(beta,eps,batch_size)\n",
        "  #   elif(optimizer==\"adam\"):\n",
        "  #     w,b=self.adam(beta1,beta2,eps,batch_size)\n",
        "  #   elif(optimizer==\"nadam\"):\n",
        "  #     w,b=self.nadam(beta1,beta2,eps,batch_size)\n",
        "  #   bestConfigRun=False\n",
        "\n",
        "  #   return w,b\n"
      ],
      "metadata": {
        "id": "qWDJV8lKK8dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accuracy:\n",
        "  def trainAccuracy(y,y_pred):\n",
        "    accuracy=0\n",
        "    for i in range(y_pred.shape[0]):\n",
        "      if(np.argmax(y[i])==np.argmax(y_pred[i])):\n",
        "        accuracy+=1\n",
        "\n",
        "    return accuracy/y.shape[0]\n",
        "\n",
        "  def valAccuracy(Model,w,b,x,y):\n",
        "    accuracy=0\n",
        "    a,h,y_pred=Model.forward_propagation(w,b,x)\n",
        "    for i in range(y_pred.shape[0]):\n",
        "      if(np.argmax(y[i])==np.argmax(y_pred)):\n",
        "        accuracy+=1\n",
        "\n",
        "    return accuracy/x.shape[0]\n",
        "\n",
        "  def testAccuracy(y,y_pred):\n",
        "    accuracy=0\n",
        "    for i in range(y.shape[0]):\n",
        "      if(np.argmax(y[i])==np.argmax(y_pred[i])):\n",
        "        accuracy+=1\n",
        "\n",
        "    return accuracy/y.shape[0]\n"
      ],
      "metadata": {
        "id": "pbl6ezQUK_b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  def trainCrossEntropyLoss(y,yPred):\n",
        "    val=-np.sum(y*np.log(yPred+1e-9))\n",
        "    return val\n",
        "\n",
        "  def valCrossEntropyLoss(Model,w,b,x,y,wd,epsilon=1e-10):\n",
        "    a,h,yPred=Model.forward_propagation(w,b,x)\n",
        "    val=-np.sum(y*np.log(yPred+1e-9))\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "mPZ9oNS-LBEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  wandb.init(project=\"Pritam CS6910 - Assignment 1\")\n",
        "  config=wandb.config\n",
        "\n",
        "  Model=FeedForwardNeuralNetwork(x_train,y_train,x_val,y_val,hls=config.number_of_hidden_layers,neurons_in_hl=config.neurons_in_each_hidden_layers,activation=config.activation_function,initialization=config.initialization_technique,epochs=config.number_of_epochs,eta=config.learning_rate,wd=config.weight_decay)\n",
        "  Model.modelFitting(beta=config.beta_value,beta1=0.9,beta2=0.999,eps=1e-5,optimizer=config.optimizer_technique,batch_size=config.batch_size,loss=config.loss_type)\n",
        "\n",
        "sweep_configuration = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'ACCURACY AND LOSS',\n",
        "    'metric': {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'validation_accuracy'\n",
        "        },\n",
        "    'parameters': {\n",
        "        'initialization_technique': {'values': ['xavier','random']},\n",
        "        'number_of_hidden_layers' : {'values' : [3,4,5]},\n",
        "        'neurons_in_each_hidden_layers' : {'values' : [32,64,128]},\n",
        "\n",
        "        'learning_rate': {'values':[1e-1,5e-1,1e-2,1e-3,5e-3,1e-4]},\n",
        "        'beta_value' : {'values' : [0.9,0.999]},\n",
        "        'optimizer_technique' : {'values' : ['sgd','momentum','rmsprop','adam','nadam','nestrov']},\n",
        "\n",
        "        'batch_size': {'values': [16,32,64,128]},\n",
        "        'number_of_epochs': {'values': [5,10]},\n",
        "        'loss_type' : {'values' : ['cross_entropy']},\n",
        "        'activation_function' : {'values' : ['sigmoid','relu','tanh']},\n",
        "        'weight_decay' : {'values' : [0, 0.0005,0.5]}\n",
        "       }\n",
        "    }\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_configuration,project='Pritam CS6910 - Assignment 1')\n",
        "\n",
        "wandb.agent(\"cs23m051/Pritam CS6910 - Assignment 1/os97numu\" , function = main , count = 150)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "DE1uqBXfLCp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7"
      ],
      "metadata": {
        "id": "hQlEljbbLIpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model=FeedForwardNeuralNetwork(x_train,y_train,x_val,y_val,hls=4,neurons_in_hl=64,activation=\"tanh\",initialization=\"xavier\",epochs=10,eta=0.001)\n",
        "w,b=Model.modelFittingForBestConfig(beta=0.9.beta,beta1=0.9,beta2=0.999,eps=1e-5,optimizer=\"nestrov\",batch_size=32,loss=\"cross_entropy\")\n",
        "\n",
        "y_test1=Model.one_hot_encoding(y_test)\n",
        "x_test1=Model.input_flattening(x_test)\n",
        "\n",
        "yPred=list()\n",
        "for i in range(len(y_test1)):\n",
        "  _,_,y_cap=Model.forward_propagation(w,b,x_test1[i])\n",
        "  yPred.append(np.array(y_cap))\n",
        "\n",
        "test_accuracy=Accuracy.testAccuracy(y_test1,yPred)\n",
        "print(\"test accuracy for the best model = {}\".format(test_accuracy))\n",
        "\n",
        "output_class=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n"
      ],
      "metadata": {
        "id": "XZYTKujmLKY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}